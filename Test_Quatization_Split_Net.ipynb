{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Parameters to be tuned:\\n- Output file name, if you don't update the name manually the previous file won't be deleted. New data will be appended to the end of the file instead.\\n- Flag that enables training data to be saved, a False flag will decrease running time significantly.\\n- Flag that enables laplacian related data to be saved.\\n- Number of simulations per layer.\\n- Total number of bits that will be flipped randomly from any weight in each layer.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple, List\n",
    "\n",
    "def evaluate_model(interpreter: tf.lite.Interpreter, test_images, test_labels) -> Tuple[float, float]:\n",
    "    \"\"\" Evaluate TFLite Model:\n",
    "    -\n",
    "    Receives the interpreter and returns a tuple of loss and accuracy.\n",
    "    \"\"\"\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    predictions = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis = 0).astype(np.float32)\n",
    "        test_image = np.expand_dims(test_image, axis = 3).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        predictions.append(np.copy(output()[0]))\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    predictions = np.array(predictions)\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()(test_labels, predictions)\n",
    "\n",
    "    loss = scce.numpy()\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return loss, accuracy\n",
    "\n",
    "def prob_mass_gen(bits : int) -> Tuple[List[int], List[float]]:\n",
    "    \"\"\" Discrete triangular distribution\n",
    "    -\n",
    "    - Receives the number of bits and calculates the probability mass function \"\"\"\n",
    "    values = np.arange(0, bits)\n",
    "    n = len(values)\n",
    "    p_max = 2/n\n",
    "    m_max = p_max/(n - 1)\n",
    "    # Probability calculation\n",
    "    p = p_max * 1.0\n",
    "    m = 2/(n - 1)*(p - 1/n)\n",
    "    probabilities = [p + m*(i - values[-1]) for i in values]\n",
    "    return values, probabilities\n",
    "\n",
    "def random_bit_flipper(value : int) -> Tuple[int, int]:\n",
    "    \"\"\" Random bit flipper \n",
    "    -\n",
    "    Obtains a value and flips one bit at a random position according to a triangular distribution.\n",
    "    - All values are in 8 bits, MSB have higher probability of getting flipped\n",
    "    - It is assumed value is a signed 8 bit number \"\"\"\n",
    "    bits, probs = prob_mass_gen(8)\n",
    "    bit_pos = np.random.choice(bits, p = probs)\n",
    "    # Negative 2 Complement conversion\n",
    "    if value < 0:\n",
    "        value = (-value ^ 0xFF) + 1\n",
    "    flip_mask = 1 << bit_pos\n",
    "    flipped_value = value ^ flip_mask\n",
    "    # Negative back conversion 2 Complement\n",
    "    if flipped_value >= 128:\n",
    "        flipped_value = -((flipped_value ^ 0xFF) + 1)\n",
    "    return bit_pos, flipped_value\n",
    "\n",
    "def random_bit_flipper_uniform(value : int) -> Tuple[int, int]:\n",
    "    \"\"\" Random bit flipper with uniform distribution\n",
    "    -\n",
    "    Obtains a value and flips one bit at a random position according to a uniform distribution.\n",
    "    - All values are in 8 bits, every bit has the same probability of getting flipped\n",
    "    - It is assumed value is a signed 8 bit number \"\"\"\n",
    "    bit_pos = np.random.randint(8)\n",
    "    # Negative 2 Complement conversion\n",
    "    if value < 0:\n",
    "        value = (-value ^ 0xFF) + 1\n",
    "    flip_mask = 1 << bit_pos\n",
    "    flipped_value = value ^ flip_mask\n",
    "    # Negative back conversion 2 Complement\n",
    "    if flipped_value >= 128:\n",
    "        flipped_value = -((flipped_value ^ 0xFF) + 1)\n",
    "    return bit_pos, flipped_value\n",
    "\n",
    "def n_bit_flipper(value : int, bit_width: int, bit_pos : int) -> int:\n",
    "    \"\"\" Bit flipper n-bit length\n",
    "    -\n",
    "    Obtains a value and flips one bit at the specified position.\n",
    "    - All values are in n bits\n",
    "    - It is assumed value is a signed n bit number \"\"\"\n",
    "    # Negative 2 Complement conversion\n",
    "    mask = 2**bit_width - 1\n",
    "    if value < 0:\n",
    "        value = (-value ^ mask) + 1\n",
    "    flip_mask = 1 << bit_pos\n",
    "    flipped_value = value ^ flip_mask\n",
    "    # Negative back conversion 2 Complement\n",
    "    if flipped_value >= 2**(bit_width - 1):\n",
    "        flipped_value = -((flipped_value ^ mask) + 1)\n",
    "    return flipped_value\n",
    "\n",
    "\"\"\" Parameters to be tuned:\n",
    "- Output file name, if you don't update the name manually the previous file won't be deleted. New data will be appended to the end of the file instead.\n",
    "- Flag that enables training data to be saved, a False flag will decrease running time significantly.\n",
    "- Flag that enables laplacian related data to be saved.\n",
    "- Number of simulations per layer.\n",
    "- Total number of bits that will be flipped randomly from any weight in each layer.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FILE_NAME = 'Performance_Multiple_5.csv'\n",
    "SAVE_TRAINING_PERFORMANCE_FLAG = False\n",
    "SAVE_LAPLACIAN_DATA_FLAG = True\n",
    "N_SIMULATIONS_PER_LAYER = 1\n",
    "N_BITS_TO_FLIP = 10\n",
    "\n",
    "MODELS_DIR = \"./model/\"\n",
    "LOAD_PATH_Q_AWARE = MODELS_DIR + \"model_q_aware_final_01\"\n",
    "LOAD_TFLITE_PATH = MODELS_DIR + 'tflite_final_01.tflite'\n",
    "SAVE_NEW_TFLITE_PATH = MODELS_DIR + 'new_tflite_flip_01.tflite'\n",
    "OUTPUTS_DIR = \"./outputs/\"\n",
    "SAVE_DATA_PATH = OUTPUTS_DIR + SAVE_FILE_NAME\n",
    "\n",
    "if not os.path.exists(OUTPUTS_DIR):\n",
    "    os.mkdir(OUTPUTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.2891 - accuracy: 0.9115\n",
      "Q Aware model test accuracy :  91.15%\n",
      "Q Aware model test loss:  0.2890910804271698\n",
      "TFLite model test accuracy: 91.17%\n",
      "TFLite model test loss:  0.3396423\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "print(train_images.shape)\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Load Q Aware model\n",
    "q_aware_model : tf.keras.Model\n",
    "with tfmot.quantization.keras.quantize_scope():\n",
    "    q_aware_model = tf.keras.models.load_model(LOAD_PATH_Q_AWARE)\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(LOAD_TFLITE_PATH)\n",
    "\n",
    "# Evaluate accuracy of both models in test set\n",
    "q_aware_test_loss, q_aware_test_acc = q_aware_model.evaluate(test_images, test_labels)\n",
    "print('Q Aware model test accuracy : ', \"{:0.2%}\".format(q_aware_test_acc))\n",
    "print('Q Aware model test loss: ', q_aware_test_loss)\n",
    "interpreter.allocate_tensors()\n",
    "tflite_test_loss, tflite_test_accuracy = evaluate_model(interpreter, test_images, test_labels)\n",
    "print('TFLite model test accuracy:', \"{:0.2%}\".format(tflite_test_accuracy))\n",
    "print('TFLite model test loss: ', tflite_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Aware model summary\n",
      "0 input_1 0 input (None, 28, 28, 1) output (None, 28, 28, 1)\n",
      "1 quantize_layer 3 input (None, 28, 28, 1) output (None, 28, 28, 1)\n",
      "2 quant_conv2d 7 input (None, 28, 28, 1) output (None, 24, 24, 32)\n",
      "3 quant_max_pooling2d 1 input (None, 24, 24, 32) output (None, 12, 12, 32)\n",
      "4 quant_conv2d_1 7 input (None, 12, 12, 32) output (None, 8, 8, 64)\n",
      "5 quant_max_pooling2d_1 1 input (None, 8, 8, 64) output (None, 4, 4, 64)\n",
      "6 quant_conv2d_2 7 input (None, 4, 4, 64) output (None, 2, 2, 96)\n",
      "7 quant_max_pooling2d_2 1 input (None, 2, 2, 96) output (None, 1, 1, 96)\n",
      "8 quant_flatten 1 input (None, 1, 1, 96) output (None, 96)\n",
      "9 quant_dense_last 7 input (None, 96) output (None, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Q Aware model summary\")\n",
    "for i, layer in enumerate(q_aware_model.layers):\n",
    "    print(i, layer.name, len(layer.variables),\"input\", layer.input.shape, \"output\", layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d/kernel:0\n",
      "conv2d/bias:0\n",
      "quant_conv2d/optimizer_step:0\n",
      "quant_conv2d/kernel_min:0\n",
      "quant_conv2d/kernel_max:0\n",
      "quant_conv2d/post_activation_min:0\n",
      "quant_conv2d/post_activation_max:0\n",
      "[-3.9335735, 4.380244]\n"
     ]
    }
   ],
   "source": [
    "TARGET_LAYER = 2\n",
    "for variable in q_aware_model.layers[TARGET_LAYER].variables:\n",
    "    print(variable.name)\n",
    "vars = [var.numpy() for var in q_aware_model.layers[TARGET_LAYER].non_trainable_variables[-2:]]\n",
    "print(vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable\n",
      "2 quant_conv2d conv2d/kernel:0\n",
      "2 quant_conv2d conv2d/bias:0\n",
      "4 quant_conv2d_1 conv2d_1/kernel:0\n",
      "4 quant_conv2d_1 conv2d_1/bias:0\n",
      "6 quant_conv2d_2 conv2d_2/kernel:0\n",
      "6 quant_conv2d_2 conv2d_2/bias:0\n",
      "9 quant_dense_last dense_last/kernel:0\n",
      "9 quant_dense_last dense_last/bias:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable\")\n",
    "for i, layer in enumerate(q_aware_model.layers):\n",
    "    for variable in layer.trainable_variables:\n",
    "        print(i, layer.name, variable.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non trainable\n",
      "1 quantize_layer quantize_layer/quantize_layer_min:0\n",
      "1 quantize_layer quantize_layer/quantize_layer_max:0\n",
      "1 quantize_layer quantize_layer/optimizer_step:0\n",
      "2 quant_conv2d quant_conv2d/optimizer_step:0\n",
      "2 quant_conv2d quant_conv2d/kernel_min:0\n",
      "2 quant_conv2d quant_conv2d/kernel_max:0\n",
      "2 quant_conv2d quant_conv2d/post_activation_min:0\n",
      "2 quant_conv2d quant_conv2d/post_activation_max:0\n",
      "3 quant_max_pooling2d quant_max_pooling2d/optimizer_step:0\n",
      "4 quant_conv2d_1 quant_conv2d_1/optimizer_step:0\n",
      "4 quant_conv2d_1 quant_conv2d_1/kernel_min:0\n",
      "4 quant_conv2d_1 quant_conv2d_1/kernel_max:0\n",
      "4 quant_conv2d_1 quant_conv2d_1/post_activation_min:0\n",
      "4 quant_conv2d_1 quant_conv2d_1/post_activation_max:0\n",
      "5 quant_max_pooling2d_1 quant_max_pooling2d_1/optimizer_step:0\n",
      "6 quant_conv2d_2 quant_conv2d_2/optimizer_step:0\n",
      "6 quant_conv2d_2 quant_conv2d_2/kernel_min:0\n",
      "6 quant_conv2d_2 quant_conv2d_2/kernel_max:0\n",
      "6 quant_conv2d_2 quant_conv2d_2/post_activation_min:0\n",
      "6 quant_conv2d_2 quant_conv2d_2/post_activation_max:0\n",
      "7 quant_max_pooling2d_2 quant_max_pooling2d_2/optimizer_step:0\n",
      "8 quant_flatten quant_flatten/optimizer_step:0\n",
      "9 quant_dense_last quant_dense_last/optimizer_step:0\n",
      "9 quant_dense_last quant_dense_last/kernel_min:0\n",
      "9 quant_dense_last quant_dense_last/kernel_max:0\n",
      "9 quant_dense_last quant_dense_last/pre_activation_min:0\n",
      "9 quant_dense_last quant_dense_last/pre_activation_max:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Non trainable\")\n",
    "for i, layer in enumerate(q_aware_model.layers):\n",
    "    for variable in layer.non_trainable_variables:\n",
    "        print(i, layer.name, variable.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input(shape = (28, 28, 1))\n",
    "conv_1 = tf.keras.layers.Conv2D(32, 5, use_bias = True, activation = 'relu')(input_layer)\n",
    "\n",
    "input_layer_2 = tf.keras.layers.Input(shape = (24, 24, 32))\n",
    "pool_1 = tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2)(input_layer_2)\n",
    "conv_2 = tf.keras.layers.Conv2D(64, 5, use_bias = True, activation = 'relu')(pool_1)\n",
    "pool_2 = tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2)(conv_2)\n",
    "conv_3 = tf.keras.layers.Conv2D(96, 3, use_bias = True, activation = 'relu')(pool_2)\n",
    "pool_3 = tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2)(conv_3)\n",
    "flat_1 = tf.keras.layers.Flatten()(pool_3)\n",
    "dense_out = tf.keras.layers.Dense(10, activation = 'softmax', name = \"dense_last\")(flat_1)\n",
    "\n",
    "nq_model_part1 = tf.keras.models.Model(inputs = input_layer, outputs = conv_1)\n",
    "nq_model_part2 = tf.keras.models.Model(inputs = input_layer_2, outputs = dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantification of values\n",
    "BIT_WIDTH = 8\n",
    "quantized_and_dequantized = OrderedDict()\n",
    "quantized = OrderedDict()\n",
    "layer_index_list = []\n",
    "keys_list = []\n",
    "layers_shapes = []\n",
    "\n",
    "layer : tfmot.quantization.keras.QuantizeWrapperV2\n",
    "for i, layer in enumerate(q_aware_model.layers):\n",
    "    quantizer : tfmot.quantization.keras.quantizers.Quantizer\n",
    "    weight : tf.Variable\n",
    "    if hasattr(layer, '_weight_vars'):\n",
    "        for weight, quantizer, quantizer_vars in layer._weight_vars:\n",
    "            min_var = quantizer_vars['min_var']\n",
    "            max_var = quantizer_vars['max_var']\n",
    "            \n",
    "            key = weight.name[:-2]\n",
    "            layer_index_list.append(i)\n",
    "            keys_list.append(key)\n",
    "            layers_shapes.append(weight.numpy().shape)\n",
    "            quantized_and_dequantized[key] = quantizer(inputs = weight, training = False, weights = quantizer_vars)\n",
    "            quantized[key] = np.round(quantized_and_dequantized[key] / max_var * (2**(BIT_WIDTH-1)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "<dtype: 'float32'>\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# Extraction of all scales and quantized values\n",
    "T_VARIABLES_KERNEL_INDEX = 0\n",
    "T_VARIABLES_BIAS_INDEX = 1\n",
    "new_layer_index_list = []\n",
    "new_keys_list = []\n",
    "input_scales = OrderedDict()\n",
    "kernel_scales = OrderedDict()\n",
    "bias_scales = OrderedDict()\n",
    "output_scales = OrderedDict()\n",
    "\n",
    "kernel_max_vars = OrderedDict()\n",
    "kernel_min_vars = OrderedDict()\n",
    "quantized_bias = OrderedDict()\n",
    "quantized_and_dequantized_weights = OrderedDict()\n",
    "quantized_weights = OrderedDict()\n",
    "\n",
    "for i, layer in enumerate(q_aware_model.layers):\n",
    "    layer_flag = False\n",
    "    for nt_variable in layer.non_trainable_variables:\n",
    "        if (\"quantize_layer\" in nt_variable.name or \"kernel\" in nt_variable.name) and not layer_flag:\n",
    "            new_layer_index_list.append(i)\n",
    "            new_keys_list.append(layer.name)\n",
    "            layer_flag = True\n",
    "\n",
    "for i, index in enumerate(new_layer_index_list):\n",
    "    key = new_keys_list[i]\n",
    "    for nt_variable in q_aware_model.layers[index].non_trainable_variables:\n",
    "        # First layer (Quantization Layer) does not have input scale\n",
    "        if i == 0:\n",
    "            if \"quantize_layer\" in nt_variable.name and \"min\" in nt_variable.name:\n",
    "                min_var = nt_variable\n",
    "            if \"quantize_layer\" in nt_variable.name and \"max\" in nt_variable.name:\n",
    "                max_var = nt_variable\n",
    "        else:\n",
    "            if \"activation_min\" in nt_variable.name:\n",
    "                min_var = nt_variable\n",
    "            if \"activation_max\" in nt_variable.name:\n",
    "                max_var = nt_variable\n",
    "            if \"kernel_min\" in nt_variable.name:\n",
    "                kernel_min_var = nt_variable\n",
    "            if \"kernel_max\" in nt_variable.name:\n",
    "                kernel_max_var = nt_variable\n",
    "    if i != 0:\n",
    "        kernel_max_vars[key] = kernel_max_var\n",
    "        kernel_min_vars[key] = kernel_min_var\n",
    "        input_scales[key] = output_scales[new_keys_list[i - 1]]\n",
    "        kernel_scales[key] = (kernel_max_var - kernel_min_var).numpy()/(2**BIT_WIDTH - 2)\n",
    "        bias_scales[key] = input_scales[key]*kernel_scales[key]\n",
    "    output_scales[key] = ((max_var - min_var).numpy()/(2**BIT_WIDTH - 1)).astype(np.float32)\n",
    "\n",
    "\n",
    "for i, index in enumerate(new_layer_index_list):\n",
    "    key = new_keys_list[i]\n",
    "    if len(q_aware_model.layers[index].trainable_variables) != 0:\n",
    "        quantized_bias[key] = np.round(q_aware_model.layers[index].trainable_variables[T_VARIABLES_BIAS_INDEX].numpy()/bias_scales[key]).astype(int)\n",
    "        if \"conv2d\" in key:\n",
    "            quantized_and_dequantized_weights[key] = tf.quantization.fake_quant_with_min_max_vars_per_channel(q_aware_model.layers[index].trainable_variables[T_VARIABLES_KERNEL_INDEX], kernel_min_vars[key], kernel_max_vars[key], BIT_WIDTH, narrow_range = True)\n",
    "        elif \"dense\" in key:\n",
    "            quantized_and_dequantized_weights[key] = tf.quantization.fake_quant_with_min_max_vars(q_aware_model.layers[index].trainable_variables[T_VARIABLES_KERNEL_INDEX], kernel_min_vars[key], kernel_max_vars[key], BIT_WIDTH, narrow_range = True)\n",
    "        quantized_weights[key] = np.round(quantized_and_dequantized_weights[key] / kernel_scales[key]).astype(int)\n",
    "        # quantized_weights[key] = np.round(q_aware_model.layers[index].trainable_variables[T_VARIABLES_KERNEL_INDEX].numpy()/kernel_scales[key]).astype(int) # Does not work properly\n",
    "\n",
    "# print(new_keys_list)\n",
    "# print(new_layer_index_list)\n",
    "\n",
    "# print(\"Input scales\", input_scales)\n",
    "# print(\"Output scales\", output_scales)\n",
    "# print(\"Kernel scales\", kernel_scales)\n",
    "# print(\"Bias scales\", bias_scales)\n",
    "\n",
    "# print(\"Quantized weights difference\", np.unique(quantized_weights[new_keys_list[4]] - quantized[keys_list[3]]))\n",
    "# print(\"Quantized weights\", quantized_weights)\n",
    "# print(\"Quantized biases\", quantized_bias)\n",
    "\n",
    "print(kernel_min_vars[new_keys_list[1]].dtype)\n",
    "print(kernel_max_vars[new_keys_list[1]].dtype)\n",
    "print(input_scales[new_keys_list[1]].dtype)\n",
    "print(kernel_scales[new_keys_list[1]].dtype)\n",
    "print(bias_scales[new_keys_list[1]].dtype)\n",
    "print(output_scales[new_keys_list[1]].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 Summary\n",
      "0 input_1 0 input (None, 28, 28, 1) output (None, 28, 28, 1)\n",
      "1 conv2d 2 input (None, 28, 28, 1) output (None, 24, 24, 32)\n",
      "Part 2 Summary\n",
      "0 input_2 0 input (None, 24, 24, 32) output (None, 24, 24, 32)\n",
      "1 max_pooling2d 0 input (None, 24, 24, 32) output (None, 12, 12, 32)\n",
      "2 conv2d_1 2 input (None, 12, 12, 32) output (None, 8, 8, 64)\n",
      "3 max_pooling2d_1 0 input (None, 8, 8, 64) output (None, 4, 4, 64)\n",
      "4 conv2d_2 2 input (None, 4, 4, 64) output (None, 2, 2, 96)\n",
      "5 max_pooling2d_2 0 input (None, 2, 2, 96) output (None, 1, 1, 96)\n",
      "6 flatten 0 input (None, 1, 1, 96) output (None, 96)\n",
      "7 dense_last 2 input (None, 96) output (None, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 1 Summary\")\n",
    "for i, layer in enumerate(nq_model_part1.layers):\n",
    "    print(i, layer.name, len(layer.variables),\"input\", layer.input.shape, \"output\", layer.output.shape)\n",
    "print(\"Part 2 Summary\")\n",
    "for i, layer in enumerate(nq_model_part2.layers):\n",
    "    print(i, layer.name, len(layer.variables),\"input\", layer.input.shape, \"output\", layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 Trainable\n",
      "1 conv2d conv2d/kernel:0\n",
      "1 conv2d conv2d/bias:0\n",
      "Part 1 Non Trainable\n",
      "Part 2 Trainable\n",
      "2 conv2d_1 conv2d_1/kernel:0\n",
      "2 conv2d_1 conv2d_1/bias:0\n",
      "4 conv2d_2 conv2d_2/kernel:0\n",
      "4 conv2d_2 conv2d_2/bias:0\n",
      "7 dense_last dense_last/kernel:0\n",
      "7 dense_last dense_last/bias:0\n",
      "Part 2 Non Trainable\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 1 Trainable\")\n",
    "for i, layer in enumerate(nq_model_part1.layers):\n",
    "    for variable in layer.trainable_variables:\n",
    "        print(i, layer.name, variable.name)\n",
    "print(\"Part 1 Non Trainable\")\n",
    "for i, layer in enumerate(nq_model_part1.layers):\n",
    "    for variable in layer.non_trainable_variables:\n",
    "        print(i, layer.name, variable.name)\n",
    "print(\"Part 2 Trainable\")\n",
    "for i, layer in enumerate(nq_model_part2.layers):\n",
    "    for variable in layer.trainable_variables:\n",
    "        print(i, layer.name, variable.name)\n",
    "print(\"Part 2 Non Trainable\")\n",
    "for i, layer in enumerate(nq_model_part2.layers):\n",
    "    for variable in layer.non_trainable_variables:\n",
    "        print(i, layer.name, variable.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 9]\n",
      "[[-0.07918215 -0.01161032  0.07245282]\n",
      " [ 0.00061669 -0.22089666 -0.01293123]\n",
      " [ 0.03844265  0.06296697  0.14747679]]\n",
      "[[-0.07918215 -0.01161032  0.07245282]\n",
      " [ 0.00061669 -0.22089666 -0.01293123]\n",
      " [ 0.03844265  0.06296697  0.14747679]]\n",
      "[ 0.02969096  0.01373224 -0.25231063 -0.01235129 -0.03170829]\n",
      "[ 0.02969096  0.01373224 -0.25231063 -0.01235129 -0.03170829]\n",
      "[[ 0.00423666 -0.10165723 -0.21178155]\n",
      " [-0.1674046  -0.30126658 -0.01047609]\n",
      " [-0.1538336  -0.02137352 -0.08994394]]\n",
      "[[ 0.00423666 -0.10165723 -0.21178155]\n",
      " [-0.1674046  -0.30126658 -0.01047609]\n",
      " [-0.1538336  -0.02137352 -0.08994394]]\n",
      "[-0.0156321  -0.01641378 -0.03496066 -0.13640228  0.00386511]\n",
      "[-0.0156321  -0.01641378 -0.03496066 -0.13640228  0.00386511]\n"
     ]
    }
   ],
   "source": [
    "print(layer_index_list)\n",
    "indexes_part1 = [1]\n",
    "indexes_part2 = [2, 4, 7]\n",
    "weights_1 = q_aware_model.layers[layer_index_list[0]].get_weights()\n",
    "weights_2 = [q_aware_model.layers[idx].get_weights() for idx in layer_index_list[1:]]\n",
    "# print([variable.name for variable in q_aware_model.layers[layer_index_list[0]].trainable_variables])\n",
    "for idx in indexes_part1:\n",
    "    nq_model_part1.layers[idx].set_weights(weights_1[:2])\n",
    "for i, idx in enumerate(indexes_part2):\n",
    "    nq_model_part2.layers[idx].set_weights(weights_2[i][:2])\n",
    "\n",
    "print(q_aware_model.layers[layer_index_list[0]].trainable_variables[0][:3,:3,0,5].numpy())\n",
    "print(nq_model_part1.layers[1].variables[0][:3,:3,0,5].numpy())\n",
    "print(q_aware_model.layers[layer_index_list[0]].trainable_variables[1][:5].numpy())\n",
    "print(nq_model_part1.layers[1].variables[1][:5].numpy())\n",
    "\n",
    "print(q_aware_model.layers[layer_index_list[2]].trainable_variables[0][:3,:3,0,15].numpy())\n",
    "print(nq_model_part2.layers[4].variables[0][:3,:3,0,15].numpy())\n",
    "print(q_aware_model.layers[layer_index_list[2]].trainable_variables[1][:5].numpy())\n",
    "print(nq_model_part2.layers[4].variables[1][:5].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape (60000, 28, 28)\n",
      "out images shape (1, 24, 24, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 650x200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02695452 0.02885798 0.00449268 0.02263477]\n",
      " [0.03401939 0.024934   0.00543544 0.        ]\n",
      " [0.04594108 0.         0.0026864  0.        ]\n",
      " [0.02412694 0.         0.         0.        ]]\n",
      "[[0.02695452 0.02885798 0.00449269 0.02263477]\n",
      " [0.0340194  0.024934   0.00543544 0.        ]\n",
      " [0.04594108 0.         0.0026864  0.        ]\n",
      " [0.02412694 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Test to replicate the behaviour of first layer\n",
    "TARGET_IMAGE = 1074\n",
    "TARGET_LAYER_FOO = 1\n",
    "T_VARIABLES_KERNEL_INDEX = 0\n",
    "T_VARIABLES_BIAS_INDEX = 1\n",
    "OUT_CHANNEL = 23\n",
    "\n",
    "foo_image = np.zeros((28,28))\n",
    "foo_image[4,4] = 1\n",
    "\n",
    "out = nq_model_part1(train_images[np.newaxis,TARGET_IMAGE])\n",
    "# out = nq_model_part1(foo_image[np.newaxis])\n",
    "\n",
    "kernel = nq_model_part1.layers[TARGET_LAYER_FOO].trainable_variables[T_VARIABLES_KERNEL_INDEX][:,:,0,OUT_CHANNEL].numpy()\n",
    "bias = nq_model_part1.layers[TARGET_LAYER_FOO].trainable_variables[T_VARIABLES_BIAS_INDEX][OUT_CHANNEL].numpy()\n",
    "\n",
    "# self_conv = sp.signal.correlate2d(foo_image, kernel, mode = \"valid\") + bias\n",
    "self_conv = sp.signal.correlate2d(train_images[TARGET_IMAGE], kernel, mode = \"valid\") + bias\n",
    "self_conv = np.maximum(0, self_conv)\n",
    "print(\"train images shape\", train_images.shape)\n",
    "print(\"out images shape\", out.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize = (6.5, 2))\n",
    "ax[0].imshow(train_images[TARGET_IMAGE], cmap = 'Greys')\n",
    "# ax[0].imshow(foo_image, cmap = 'Greys')\n",
    "ax[0].set_title('original input')\n",
    "ax[0].axis('equal')\n",
    "ax[0].set(xlim = (0, train_images.shape[-2:][0]), ylim = (train_images.shape[-2:][1], 0))\n",
    "\n",
    "ax[1].imshow(out[0,:,:,OUT_CHANNEL], cmap = 'Greys')\n",
    "ax[1].set_title('first layer output')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlim = (0, out.shape[1:3][0]), ylim = (out.shape[1:3][1], 0))\n",
    "\n",
    "ax[2].imshow(self_conv, cmap = 'Greys')\n",
    "ax[2].set_title('self convolution')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set(xlim = (0, out.shape[1:3][0]), ylim = (out.shape[1:3][1], 0))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "slicer = slice(None,4)\n",
    "print(self_conv[(slicer,slicer)])\n",
    "print(out[(0,slicer,slicer,OUT_CHANNEL)].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.]\n"
     ]
    }
   ],
   "source": [
    "KERNEL_INDEX = 0\n",
    "BIAS_INDEX = 1\n",
    "original_layer_vars = [[variable.numpy() for variable in q_aware_model.layers[idx].trainable_variables] for idx in layer_index_list]\n",
    "\n",
    "print(np.unique((nq_model_part1.layers[indexes_part1[0]].variables[KERNEL_INDEX] - original_layer_vars[0][KERNEL_INDEX]).numpy()),\n",
    "np.unique((nq_model_part1.layers[indexes_part1[0]].variables[BIAS_INDEX] - original_layer_vars[0][BIAS_INDEX]).numpy()),\n",
    "\n",
    "np.unique((nq_model_part2.layers[indexes_part2[0]].variables[KERNEL_INDEX] - original_layer_vars[1][KERNEL_INDEX]).numpy()),\n",
    "np.unique((nq_model_part2.layers[indexes_part2[0]].variables[BIAS_INDEX] - original_layer_vars[1][BIAS_INDEX]).numpy()),\n",
    "\n",
    "np.unique((nq_model_part2.layers[indexes_part2[1]].variables[KERNEL_INDEX] - original_layer_vars[2][KERNEL_INDEX]).numpy()),\n",
    "np.unique((nq_model_part2.layers[indexes_part2[1]].variables[BIAS_INDEX] - original_layer_vars[2][BIAS_INDEX]).numpy()),\n",
    "\n",
    "np.unique((nq_model_part2.layers[indexes_part2[1]].variables[KERNEL_INDEX] - original_layer_vars[2][KERNEL_INDEX]).numpy()),\n",
    "np.unique((nq_model_part2.layers[indexes_part2[1]].variables[BIAS_INDEX] - original_layer_vars[2][BIAS_INDEX]).numpy()),\n",
    "\n",
    "np.unique((nq_model_part2.layers[indexes_part2[2]].variables[KERNEL_INDEX] - original_layer_vars[3][KERNEL_INDEX]).numpy()),\n",
    "np.unique((nq_model_part2.layers[indexes_part2[2]].variables[BIAS_INDEX] - original_layer_vars[3][BIAS_INDEX]).numpy()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Part 1 Summary\n",
      "0 input_1 vars 0 input (None, 28, 28, 1) output (None, 28, 28, 1)\n",
      "1 quantize_layer vars 3 input (None, 28, 28, 1) output (None, 28, 28, 1)\n",
      "2 quant_conv2d vars 7 input (None, 28, 28, 1) output (None, 24, 24, 32)\n",
      "Q Part 2 Summary\n",
      "0 input_2 vars 0 input (None, 24, 24, 32) output (None, 24, 24, 32)\n",
      "1 quantize_layer_1 vars 3 input (None, 24, 24, 32) output (None, 24, 24, 32)\n",
      "2 quant_max_pooling2d vars 1 input (None, 24, 24, 32) output (None, 12, 12, 32)\n",
      "3 quant_conv2d_1 vars 7 input (None, 12, 12, 32) output (None, 8, 8, 64)\n",
      "4 quant_max_pooling2d_1 vars 1 input (None, 8, 8, 64) output (None, 4, 4, 64)\n",
      "5 quant_conv2d_2 vars 7 input (None, 4, 4, 64) output (None, 2, 2, 96)\n",
      "6 quant_max_pooling2d_2 vars 1 input (None, 2, 2, 96) output (None, 1, 1, 96)\n",
      "7 quant_flatten vars 1 input (None, 1, 1, 96) output (None, 96)\n",
      "8 quant_dense_last vars 7 input (None, 96) output (None, 10)\n"
     ]
    }
   ],
   "source": [
    "q_model_part1 = tfmot.quantization.keras.quantize_model(nq_model_part1)\n",
    "q_model_part2 = tfmot.quantization.keras.quantize_model(nq_model_part2)\n",
    "print(\"Q Part 1 Summary\")\n",
    "for i, layer in enumerate(q_model_part1.layers):\n",
    "    print(i, layer.name, \"vars\", len(layer.variables),\"input\", layer.input.shape, \"output\", layer.output.shape)\n",
    "print(\"Q Part 2 Summary\")\n",
    "for i, layer in enumerate(q_model_part2.layers):\n",
    "    print(i, layer.name, \"vars\", len(layer.variables),\"input\", layer.input.shape, \"output\", layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 Trainable\n",
      "2 quant_conv2d conv2d/kernel:0\n",
      "2 quant_conv2d conv2d/bias:0\n",
      "Part 2 Trainable\n",
      "3 quant_conv2d_1 conv2d_1/kernel:0\n",
      "3 quant_conv2d_1 conv2d_1/bias:0\n",
      "5 quant_conv2d_2 conv2d_2/kernel:0\n",
      "5 quant_conv2d_2 conv2d_2/bias:0\n",
      "8 quant_dense_last dense_last/kernel:0\n",
      "8 quant_dense_last dense_last/bias:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 1 Trainable\")\n",
    "for i, layer in enumerate(q_model_part1.layers):\n",
    "    for variable in layer.trainable_variables:\n",
    "        print(i, layer.name, variable.name)\n",
    "print(\"Part 2 Trainable\")\n",
    "for i, layer in enumerate(q_model_part2.layers):\n",
    "    for variable in layer.trainable_variables:\n",
    "        print(i, layer.name, variable.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 Non Trainable\n",
      "1 quantize_layer quantize_layer/quantize_layer_min:0\n",
      "1 quantize_layer quantize_layer/quantize_layer_max:0\n",
      "1 quantize_layer quantize_layer/optimizer_step:0\n",
      "2 quant_conv2d quant_conv2d/optimizer_step:0\n",
      "2 quant_conv2d quant_conv2d/kernel_min:0\n",
      "2 quant_conv2d quant_conv2d/kernel_max:0\n",
      "2 quant_conv2d quant_conv2d/post_activation_min:0\n",
      "2 quant_conv2d quant_conv2d/post_activation_max:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 1 Non Trainable\")\n",
    "for i, layer in enumerate(q_model_part1.layers):\n",
    "    for variable in layer.non_trainable_variables:\n",
    "        print(i, layer.name, variable.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 Non Trainable\n",
      "1 quantize_layer_1 quantize_layer_1/quantize_layer_1_min:0\n",
      "1 quantize_layer_1 quantize_layer_1/quantize_layer_1_max:0\n",
      "1 quantize_layer_1 quantize_layer_1/optimizer_step:0\n",
      "2 quant_max_pooling2d quant_max_pooling2d/optimizer_step:0\n",
      "3 quant_conv2d_1 quant_conv2d_1/optimizer_step:0\n",
      "3 quant_conv2d_1 quant_conv2d_1/kernel_min:0\n",
      "3 quant_conv2d_1 quant_conv2d_1/kernel_max:0\n",
      "3 quant_conv2d_1 quant_conv2d_1/post_activation_min:0\n",
      "3 quant_conv2d_1 quant_conv2d_1/post_activation_max:0\n",
      "4 quant_max_pooling2d_1 quant_max_pooling2d_1/optimizer_step:0\n",
      "5 quant_conv2d_2 quant_conv2d_2/optimizer_step:0\n",
      "5 quant_conv2d_2 quant_conv2d_2/kernel_min:0\n",
      "5 quant_conv2d_2 quant_conv2d_2/kernel_max:0\n",
      "5 quant_conv2d_2 quant_conv2d_2/post_activation_min:0\n",
      "5 quant_conv2d_2 quant_conv2d_2/post_activation_max:0\n",
      "6 quant_max_pooling2d_2 quant_max_pooling2d_2/optimizer_step:0\n",
      "7 quant_flatten quant_flatten/optimizer_step:0\n",
      "8 quant_dense_last quant_dense_last/optimizer_step:0\n",
      "8 quant_dense_last quant_dense_last/kernel_min:0\n",
      "8 quant_dense_last quant_dense_last/kernel_max:0\n",
      "8 quant_dense_last quant_dense_last/pre_activation_min:0\n",
      "8 quant_dense_last quant_dense_last/pre_activation_max:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 2 Non Trainable\")\n",
    "for i, layer in enumerate(q_model_part2.layers):\n",
    "    for variable in layer.non_trainable_variables:\n",
    "        print(i, layer.name, variable.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.] [0.] [0] [0.] [0.] [0] [0.] [0.] [0.] [0.] [-3.9335735] [4.380244] [-1] [0] [0.] [0.] [0] [0.] [0.] [0.] [0.] [0] [0.] [0.] [0] [0.] [0.] [0.] [0.] [0] [0] [0.] [0.] [0] [0.] [0.] [0.] [0.]\n"
     ]
    }
   ],
   "source": [
    "# Assignation of values to first divided model both parts quantized\n",
    "q_indexes_part1 = [1, 2]\n",
    "q_indexes_part2 = list(range(3, 9 + 1))\n",
    "q_indexes_part2_2 = list(range(2, 8 + 1))\n",
    "# for i, layer in enumerate(q_aware_model.layers):\n",
    "#     # layer.get_weights()\n",
    "#     if len(layer.get_weights()) != 0:\n",
    "#         print(i)\n",
    "q_weights_1 = [q_aware_model.layers[idx].get_weights() for idx in q_indexes_part1]\n",
    "q_weights_2 = [q_aware_model.layers[idx].get_weights() for idx in q_indexes_part2]\n",
    "# print(q_weights_1[0])\n",
    "for i, idx in enumerate(q_indexes_part1):\n",
    "    q_model_part1.layers[idx].set_weights(q_weights_1[i])\n",
    "intermediate_max_min = q_aware_model.layers[2].get_weights()[-2:]\n",
    "intermediate_max_min.append(-1)\n",
    "q_model_part2.layers[1].set_weights(intermediate_max_min)\n",
    "for i, idx in enumerate(q_indexes_part2_2):\n",
    "    q_model_part2.layers[idx].set_weights(q_weights_2[i])\n",
    "\n",
    "KERNEL_INDEX = 0\n",
    "BIAS_INDEX = 1\n",
    "# print(q_model_part1.layers[q_indexes_part1[1]].variables)\n",
    "# print(q_aware_model.layers[q_indexes_part1[1]].variables)\n",
    "print(np.unique((q_model_part1.layers[q_indexes_part1[0]].variables[0] - q_aware_model.layers[q_indexes_part1[0]].variables[0]).numpy()),\n",
    "np.unique((q_model_part1.layers[q_indexes_part1[0]].variables[1] - q_aware_model.layers[q_indexes_part1[0]].variables[1]).numpy()),\n",
    "np.unique((q_model_part1.layers[q_indexes_part1[0]].variables[2] - q_aware_model.layers[q_indexes_part1[0]].variables[2]).numpy()),\n",
    "\n",
    "np.unique((q_model_part1.layers[q_indexes_part1[1]].variables[0] - q_aware_model.layers[q_indexes_part1[1]].variables[0]).numpy()),\n",
    "np.unique((q_model_part1.layers[q_indexes_part1[1]].variables[1] - q_aware_model.layers[q_indexes_part1[1]].variables[1]).numpy()),\n",
    "np.unique((q_model_part1.layers[q_indexes_part1[1]].variables[2] - q_aware_model.layers[q_indexes_part1[1]].variables[2]).numpy()),\n",
    "np.unique((q_model_part1.layers[q_indexes_part1[1]].variables[3] - q_aware_model.layers[q_indexes_part1[1]].variables[3]).numpy()),\n",
    "np.unique((q_model_part1.layers[q_indexes_part1[1]].variables[4] - q_aware_model.layers[q_indexes_part1[1]].variables[4]).numpy()),\n",
    "np.unique((q_model_part1.layers[q_indexes_part1[1]].variables[5] - q_aware_model.layers[q_indexes_part1[1]].variables[5]).numpy()),\n",
    "np.unique((q_model_part1.layers[q_indexes_part1[1]].variables[6] - q_aware_model.layers[q_indexes_part1[1]].variables[6]).numpy()),\n",
    "\n",
    "np.unique(q_model_part2.layers[1].variables[0].numpy()),\n",
    "np.unique(q_model_part2.layers[1].variables[1].numpy()),\n",
    "np.unique(q_model_part2.layers[1].variables[2].numpy()),\n",
    "\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[0]].variables[0] - q_aware_model.layers[q_indexes_part2[0]].variables[0]).numpy()),\n",
    "\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[1]].variables[0] - q_aware_model.layers[q_indexes_part2[1]].variables[0]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[1]].variables[1] - q_aware_model.layers[q_indexes_part2[1]].variables[1]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[1]].variables[2] - q_aware_model.layers[q_indexes_part2[1]].variables[2]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[1]].variables[3] - q_aware_model.layers[q_indexes_part2[1]].variables[3]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[1]].variables[4] - q_aware_model.layers[q_indexes_part2[1]].variables[4]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[1]].variables[5] - q_aware_model.layers[q_indexes_part2[1]].variables[5]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[1]].variables[6] - q_aware_model.layers[q_indexes_part2[1]].variables[6]).numpy()),\n",
    "\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[2]].variables[0] - q_aware_model.layers[q_indexes_part2[2]].variables[0]).numpy()),\n",
    "\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[3]].variables[0] - q_aware_model.layers[q_indexes_part2[3]].variables[0]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[3]].variables[1] - q_aware_model.layers[q_indexes_part2[3]].variables[1]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[3]].variables[2] - q_aware_model.layers[q_indexes_part2[3]].variables[2]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[3]].variables[3] - q_aware_model.layers[q_indexes_part2[3]].variables[3]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[3]].variables[4] - q_aware_model.layers[q_indexes_part2[3]].variables[4]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[3]].variables[5] - q_aware_model.layers[q_indexes_part2[3]].variables[5]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[3]].variables[6] - q_aware_model.layers[q_indexes_part2[3]].variables[6]).numpy()),\n",
    "\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[4]].variables[0] - q_aware_model.layers[q_indexes_part2[4]].variables[0]).numpy()),\n",
    "\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[5]].variables[0] - q_aware_model.layers[q_indexes_part2[5]].variables[0]).numpy()),\n",
    "\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[6]].variables[0] - q_aware_model.layers[q_indexes_part2[6]].variables[0]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[6]].variables[1] - q_aware_model.layers[q_indexes_part2[6]].variables[1]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[6]].variables[2] - q_aware_model.layers[q_indexes_part2[6]].variables[2]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[6]].variables[3] - q_aware_model.layers[q_indexes_part2[6]].variables[3]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[6]].variables[4] - q_aware_model.layers[q_indexes_part2[6]].variables[4]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[6]].variables[5] - q_aware_model.layers[q_indexes_part2[6]].variables[5]).numpy()),\n",
    "np.unique((q_model_part2.layers[q_indexes_part2_2[6]].variables[6] - q_aware_model.layers[q_indexes_part2[6]].variables[6]).numpy()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original output\n",
      " [[1.8473416e-09 9.9999988e-01 2.5903853e-08 4.0264447e-10 5.8375569e-08\n",
      "  5.6541989e-12 1.2723472e-08 5.6541989e-12 2.6821853e-10 5.6541989e-12]]\n",
      "non quantized model output\n",
      " [[1.4625762e-15 1.0000000e+00 7.5149568e-15 4.2900803e-17 9.2893006e-14\n",
      "  1.2390763e-24 2.8898564e-15 5.6050442e-23 1.1157035e-16 4.4603877e-22]]\n",
      "quantized model output\n",
      " [[1.8473416e-09 9.9999988e-01 2.5903853e-08 4.0264447e-10 5.8375569e-08\n",
      "  5.6541989e-12 1.2723472e-08 5.6541989e-12 2.6821853e-10 5.6541989e-12]]\n",
      "\n",
      "Unique difference non quantized [-1.1920929e-07  5.6541989e-12  2.6821842e-10  4.0264442e-10\n",
      "  1.8473402e-09  1.2723469e-08  2.5903846e-08  5.8375477e-08]\n",
      "Unique difference quantized [0.]\n"
     ]
    }
   ],
   "source": [
    "# Test compare inference both original and parts-model\n",
    "TARGET_IMAGE = 2100\n",
    "out_original = q_aware_model(train_images[np.newaxis,TARGET_IMAGE])\n",
    "\n",
    "# Does not yield the same results\n",
    "out_1 = nq_model_part1(train_images[np.newaxis,TARGET_IMAGE])\n",
    "out_2 = nq_model_part2(out_1)\n",
    "\n",
    "# Yields the same results\n",
    "out_3 = q_model_part1(train_images[np.newaxis,TARGET_IMAGE])\n",
    "out_4 = q_model_part2(out_3)\n",
    "\n",
    "print(\"original output\\n\", out_original .numpy())\n",
    "print(\"non quantized model output\\n\", out_2.numpy())\n",
    "print(\"quantized model output\\n\", out_4.numpy())\n",
    "print(\"\\nUnique difference non quantized\", np.unique((out_original - out_2).numpy()))\n",
    "print(\"Unique difference quantized\", np.unique((out_original - out_4).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantize_layer\n",
      "quant_conv2d\n",
      "input scale 0.00392156862745098\n",
      "input zero -128\n",
      "kernel scales [0.00329171 0.00318886 0.00154461 0.00221212 0.00216033 0.00173925\n",
      " 0.00257462 0.00135561 0.00259043 0.00228444 0.01463084 0.00207081\n",
      " 0.00626407 0.00238244 0.01474832 0.00248384 0.00260051 0.00279522\n",
      " 0.00929473 0.002684   0.00207167 0.0020169  0.00217691 0.00185296\n",
      " 0.00304491 0.00207702 0.0010245  0.00203166 0.00265791 0.00135298\n",
      " 0.00273699 0.00247921]\n",
      "bias scales [1.2908661e-05 1.2505331e-05 6.0573111e-06 8.6749860e-06 8.4718740e-06\n",
      " 6.8206023e-06 1.0096539e-05 5.3161089e-06 1.0158549e-05 8.9585819e-06\n",
      " 5.7375863e-05 8.1208136e-06 2.4564994e-05 9.3428880e-06 5.7836533e-05\n",
      " 9.7405646e-06 1.0198080e-05 1.0961662e-05 3.6449925e-05 1.0525496e-05\n",
      " 8.1241951e-06 7.9094089e-06 8.5368883e-06 7.2664952e-06 1.1940828e-05\n",
      " 8.1451790e-06 4.0176528e-06 7.9673073e-06 1.0423187e-05 5.3058229e-06\n",
      " 1.0733294e-05 9.7224120e-06]\n",
      "activation scale 0.03260320401659199\n",
      "activation zero -7\n",
      "bias [  2300   1098 -41654  -1424  -3743 -19204  -1805   3815  -1373  -1241\n",
      "   4801   -235   4490  -1226   5328  -3315    819   1959   4093   -688\n",
      "  -2146   1330  -3142   3709  -2705  18189 -55454  -1871     36 -43575\n",
      "   1414   -575]\n",
      "bias ['0x8fc', '0x44a', '0xffff5d4a', '0xfffffa70', '0xfffff161', '0xffffb4fc', '0xfffff8f3', '0xee7', '0xfffffaa3', '0xfffffb27', '0x12c1', '0xffffff15', '0x118a', '0xfffffb36', '0x14d0', '0xfffff30d', '0x333', '0x7a7', '0xffd', '0xfffffd50', '0xfffff79e', '0x532', '0xfffff3ba', '0xe7d', '0xfffff56f', '0x470d', '0xffff2762', '0xfffff8b1', '0x24', '0xffff55c9', '0x586', '0xfffffdc1']\n"
     ]
    }
   ],
   "source": [
    "# Extraction of scales and bias of first layer calculation\n",
    "INPUT_LAYER = 1\n",
    "CONV1_LAYER = 2\n",
    "BIT_WIDTH = 8\n",
    "BIAS_BIT_WIDTH = 32\n",
    "T_VARIABLES_KERNEL_INDEX = 0\n",
    "T_VARIABLES_BIAS_INDEX = 1\n",
    "\n",
    "print(q_aware_model.layers[INPUT_LAYER].name)\n",
    "vars_input = {variable.name: variable.numpy() for i, variable in enumerate(q_aware_model.layers[INPUT_LAYER].non_trainable_variables) if \"min\" in variable.name or \"max\" in variable.name}\n",
    "# print(vars_input)\n",
    "quantize_keys_input = [key for key in vars_input if \"quantize\" in key]\n",
    "kernel_keys_input = [key for key in vars_input if \"kernel\" in key]\n",
    "activation_keys_input = [key for key in vars_input if \"activation\" in key]\n",
    "if len(quantize_keys_input) != 0:\n",
    "    min_quantize_key_input, = tuple(key for key in quantize_keys_input if \"min\" in key)\n",
    "    max_quantize_key_input, = tuple(key for key in quantize_keys_input if \"max\" in key)\n",
    "else:\n",
    "    min_quantize_key_input = None\n",
    "    max_quantize_key_input = None\n",
    "if len(kernel_keys_input) != 0:\n",
    "    min_kernel_key_input, = tuple(key for key in kernel_keys_input if \"min\" in key)\n",
    "    max_kernel_key_input, = tuple(key for key in kernel_keys_input if \"max\" in key)\n",
    "else:\n",
    "    min_kernel_key_input = None\n",
    "    max_kernel_key_input = None\n",
    "if len(activation_keys_input) != 0:\n",
    "    min_activ_key_input, = tuple(key for key in activation_keys_input if \"min\" in key)\n",
    "    max_activ_key_input, = tuple(key for key in activation_keys_input if \"max\" in key)\n",
    "else:\n",
    "    min_activ_key_input = None\n",
    "    max_activ_key_input = None\n",
    "\n",
    "print(q_aware_model.layers[CONV1_LAYER].name)\n",
    "vars_conv1 = {variable.name: variable.numpy() for i, variable in enumerate(q_aware_model.layers[CONV1_LAYER].non_trainable_variables) if \"min\" in variable.name or \"max\" in variable.name}\n",
    "# print(vars_conv1)\n",
    "quantize_keys_conv1 = [key for key in vars_conv1 if \"quantize\" in key]\n",
    "kernel_keys_conv1 = [key for key in vars_conv1 if \"kernel\" in key]\n",
    "activation_keys_conv1 = [key for key in vars_conv1 if \"activation\" in key]\n",
    "if len(quantize_keys_conv1) != 0:\n",
    "    min_quantize_key_conv1, = tuple(key for key in quantize_keys_conv1 if \"min\" in key)\n",
    "    max_quantize_key_conv1, = tuple(key for key in quantize_keys_conv1 if \"max\" in key)\n",
    "else:\n",
    "    min_quantize_key_conv1 = None\n",
    "    max_quantize_key_conv1 = None\n",
    "if len(kernel_keys_conv1) != 0:\n",
    "    min_kernel_key_conv1, = tuple(key for key in kernel_keys_conv1 if \"min\" in key)\n",
    "    max_kernel_key_conv1, = tuple(key for key in kernel_keys_conv1 if \"max\" in key)\n",
    "else:\n",
    "    min_kernel_key_conv1 = None\n",
    "    max_kernel_key_conv1 = None\n",
    "if len(activation_keys_conv1) != 0:\n",
    "    min_activ_key_conv1, = tuple(key for key in activation_keys_conv1 if \"min\" in key)\n",
    "    max_activ_key_conv1, = tuple(key for key in activation_keys_conv1 if \"max\" in key)\n",
    "else:\n",
    "    min_activ_key_conv1 = None\n",
    "    max_activ_key_conv1 = None\n",
    "\n",
    "input_min = vars_input.get(min_quantize_key_input)\n",
    "input_max = vars_input.get(max_quantize_key_input)\n",
    "kernel_min = vars_conv1.get(min_kernel_key_conv1)\n",
    "kernel_max = vars_conv1.get(max_kernel_key_conv1)\n",
    "activation_min = vars_conv1.get(min_activ_key_conv1)\n",
    "activation_max = vars_conv1.get(max_activ_key_conv1)\n",
    "\n",
    "# Input scales and zeros\n",
    "input_scale = (input_max - input_min) / (2**BIT_WIDTH - 1)\n",
    "input_zero = np.round(2**(BIT_WIDTH - 1) - 1 - input_max/input_scale).astype(int)\n",
    "# Kernel and biases scales\n",
    "# kernel_scales = (kernel_max - kernel_min) / (2**BIT_WIDTH - 2)\n",
    "kernel_scales = kernel_max / (2**(BIT_WIDTH - 1) - 1)\n",
    "bias_scales_first_layer = input_scale * kernel_scales\n",
    "# Output scale and zero\n",
    "activation_scale = (activation_max - activation_min) / (2**BIT_WIDTH - 1)\n",
    "activation_zero = np.round(2**(BIT_WIDTH - 1) - 1 - activation_max/activation_scale).astype(int)\n",
    "\n",
    "print(\"input scale\", input_scale)\n",
    "print(\"input zero\", input_zero)\n",
    "print(\"kernel scales\", kernel_scales)\n",
    "print(\"bias scales\", bias_scales_first_layer)\n",
    "print(\"activation scale\", activation_scale)\n",
    "print(\"activation zero\", activation_zero)\n",
    "\n",
    "bias_ints = np.round(q_aware_model.layers[CONV1_LAYER].trainable_variables[T_VARIABLES_BIAS_INDEX].numpy() / bias_scales_first_layer).astype(int)\n",
    "print(\"bias\", bias_ints)\n",
    "print(\"bias\", [hex(bias) if bias > 0 else hex((-bias ^ 0xFFFFFFFF) + 1) for bias in bias_ints])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-128   33   19   32   42]\n",
      " [-126   47   18   17   40]\n",
      " [ -93   49   27   12   23]\n",
      " [ -69   42   25   13   -8]\n",
      " [ -49   17   32   86   -5]]\n",
      "Unique difference between image and quantized and dequantized image [0.00000000e+00 2.77555756e-17 5.55111512e-17 1.11022302e-16]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test compare inference both original and parts-model\n",
    "TARGET_IMAGE = 10\n",
    "TARGET_LAYER_FOO = 1\n",
    "T_VARIABLES_KERNEL_INDEX = 0\n",
    "T_VARIABLES_BIAS_INDEX = 1\n",
    "OUT_CHANNEL = 29\n",
    "slicer = slice(4,9)\n",
    "quantized_input_image = np.round(train_images[np.newaxis,TARGET_IMAGE]/input_scale).astype(int) + input_zero\n",
    "quantized_and_dequantized_input = input_scale*(quantized_input_image - input_zero)\n",
    "\n",
    "print(quantized_input_image[0,slicer,slicer])\n",
    "print(\"Unique difference between image and quantized and dequantized image\", np.unique(train_images[TARGET_IMAGE] - quantized_and_dequantized_input[0,:,:]))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (4.5, 2))\n",
    "\n",
    "ax[0].imshow(train_images[TARGET_IMAGE], cmap = 'Greys')\n",
    "ax[0].set_title('Non quantized model')\n",
    "ax[0].axis('equal')\n",
    "ax[0].set(xlim = (0, out.shape[1:3][0]), ylim = (out.shape[1:3][1], 0))\n",
    "\n",
    "ax[1].imshow(quantized_and_dequantized_input[0,:,:], cmap = 'Greys')\n",
    "ax[1].set_title('Quantized dequantized model')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlim = (0, out.shape[1:3][0]), ylim = (out.shape[1:3][1], 0))\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.0326032  0.0326032 ]\n",
      " [0.         0.         0.         0.         0.0326032 ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.16301602 0.16301602 0.06520641 0.         0.        ]\n",
      " [0.26082563 0.29342884 0.19561923 0.13041282 0.        ]]\n",
      "[[0.         0.         0.         0.01980133 0.03946471]\n",
      " [0.         0.         0.         0.         0.01970583]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.15172531 0.17578722 0.07327872 0.00453648 0.        ]\n",
      " [0.24928348 0.3046126  0.20572267 0.12452766 0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test outputs with quantization and NO post activation quantization\n",
    "out_original = q_aware_model(train_images[np.newaxis,TARGET_IMAGE])\n",
    "\n",
    "# Two part model output\n",
    "out_3 = q_model_part1(train_images[np.newaxis,TARGET_IMAGE])\n",
    "out_4 = q_model_part2(out_3)\n",
    "\n",
    "# First layer\n",
    "key = keys_list[0]\n",
    "\n",
    "kernel = quantized[key][:,:,0,OUT_CHANNEL]\n",
    "bias = bias_ints[OUT_CHANNEL]\n",
    "\n",
    "# self_conv = sp.signal.correlate2d(foo_image, kernel, mode = \"valid\") + bias\n",
    "self_conv = sp.signal.correlate2d(quantized_input_image[0] - input_zero, kernel, mode = \"valid\").astype(int) + bias\n",
    "self_conv = np.maximum(0, self_conv)\n",
    "\n",
    "self_conv_float = bias_scales_first_layer[OUT_CHANNEL]*self_conv\n",
    "print(out_3[0,slicer,slicer,OUT_CHANNEL].numpy())\n",
    "print(self_conv_float[slicer,slicer])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (4.5, 2))\n",
    "\n",
    "ax[0].imshow(out_3[0,:,:,OUT_CHANNEL], cmap = 'Greys')\n",
    "ax[0].set_title('Quantized model')\n",
    "ax[0].axis('equal')\n",
    "ax[0].set(xlim = (0, out.shape[1:3][0]), ylim = (out.shape[1:3][1], 0))\n",
    "\n",
    "ax[1].imshow(self_conv_float[:,:], cmap = 'Greys')\n",
    "ax[1].set_title('Self quantized model with scipy')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlim = (0, out.shape[1:3][0]), ylim = (out.shape[1:3][1], 0))\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model\n",
      "[[0.         0.         0.13041282 0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.0326032  0.         0.        ]\n",
      " [0.         0.22822243 0.         0.        ]]\n",
      "Self quantized model no activation\n",
      "[[0.         0.         0.14309532 0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.03776719 0.         0.        ]\n",
      " [0.         0.21986756 0.         0.        ]]\n",
      "Difference between original input no normalization and quantized input\n",
      "[0]\n",
      "Self quantized with activation\n",
      "[[0.         0.         0.13041282 0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.0326032  0.         0.        ]\n",
      " [0.         0.22822243 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Two part model output Test against quantized model with post activation quantization included\n",
    "TARGET_IMAGE = 5300 #100\n",
    "OUT_CHANNEL = 22 #20\n",
    "\n",
    "(train_images_2, train_labels_2), (test_images_2, test_labels_2) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "quantized_input_image = np.round(train_images[TARGET_IMAGE]/input_scale).astype(int) + input_zero\n",
    "quantized_and_dequantized_input = input_scale*(quantized_input_image - input_zero)\n",
    "\n",
    "out_nq = nq_model_part1(train_images[np.newaxis,TARGET_IMAGE])\n",
    "out_q = q_model_part1(train_images[np.newaxis,TARGET_IMAGE])\n",
    "\n",
    "increment = 4\n",
    "initial_x = 5\n",
    "initial_y = 3\n",
    "sub_pos_x = (initial_x, initial_x + increment)\n",
    "sub_pos_y = (initial_y, initial_y + increment)\n",
    "position = (0, slice(*sub_pos_y), slice(*sub_pos_x), OUT_CHANNEL)\n",
    "# print(\"Original input\")\n",
    "# print(train_images[TARGET_IMAGE, slice(*sub_pos_y), slice(*sub_pos_x)])\n",
    "# print(\"Quantized input\")\n",
    "# print(quantized_and_dequantized_input[slice(*sub_pos_y), slice(*sub_pos_x)])\n",
    "print(\"Quantized model\")\n",
    "print(out_q[position].numpy())\n",
    "\n",
    "# First layer\n",
    "key = keys_list[0]\n",
    "kernel = quantized[key][:,:,0]\n",
    "bias = bias_ints\n",
    "# print(kernel.shape)\n",
    "# print(bias.shape)\n",
    "\n",
    "conv_array = []\n",
    "for channel in range(kernel.shape[-1]):\n",
    "    self_conv = sp.signal.correlate2d(quantized_input_image - input_zero, kernel[:,:,channel], mode = \"valid\").astype(int) + bias[channel]\n",
    "    self_conv = np.maximum(0, self_conv)\n",
    "    float_conv = bias_scales_first_layer[channel]*self_conv\n",
    "    conv_array.append(float_conv)\n",
    "out_conv = np.array(conv_array)\n",
    "out_conv = np.moveaxis(out_conv, 0, 2)\n",
    "\n",
    "print(\"Self quantized model no activation\")\n",
    "print(out_conv[position[1:]])\n",
    "\n",
    "print(\"Difference between original input no normalization and quantized input\")\n",
    "print(np.unique(train_images_2[TARGET_IMAGE] - (quantized_input_image - input_zero)))\n",
    "\n",
    "out_conv_3 = out_conv\n",
    "out_conv_3 = activation_scale * np.round(out_conv_3 / activation_scale)\n",
    "print(\"Self quantized with activation\")\n",
    "print(out_conv_3[position[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed model practice First part no bias, Second part with bias\n",
    "input_layer = tf.keras.layers.Input(shape = (28, 28, 1))\n",
    "conv_1_1 = tf.keras.layers.Conv2D(32, 5, use_bias = False, activation = None)(input_layer)\n",
    "conv_1_1_2 = tf.keras.layers.Conv2D(32, 5, use_bias = True, activation = 'relu')(input_layer)\n",
    "\n",
    "nq2_model_part1 = tf.keras.models.Model(inputs = input_layer, outputs = conv_1_1)\n",
    "nq2_model2_part1 = tf.keras.models.Model(inputs = input_layer, outputs = conv_1_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 Summary\n",
      "0 input_3 0 input (None, 28, 28, 1) output (None, 28, 28, 1)\n",
      "1 conv2d_3 1 input (None, 28, 28, 1) output (None, 24, 24, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 1 Summary\")\n",
    "for i, layer in enumerate(nq2_model_part1.layers):\n",
    "    print(i, layer.name, len(layer.variables),\"input\", layer.input.shape, \"output\", layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non quantized model with quantized weights and quantized input\n",
      "[[-35042.   9846.  19631.  20150.]\n",
      " [-40184.  12607.  10848.  15334.]\n",
      " [-45388.  29371.   2536.   8055.]\n",
      " [-31863.  31451.   2903.   4584.]]\n",
      "Self quantized\n",
      "[[-35042   9846  19631  20150]\n",
      " [-40184  12607  10848  15334]\n",
      " [-45388  29371   2536   8055]\n",
      " [-31863  31451   2903   4584]]\n",
      "Self quantized with biases\n",
      "[[    0  6704 16489 17008]\n",
      " [    0  9465  7706 12192]\n",
      " [    0 26229     0  4913]\n",
      " [    0 28309     0  1442]]\n",
      "Layers usage\n",
      "[[    0.  6704. 16489. 17008.]\n",
      " [    0.  9465.  7706. 12192.]\n",
      " [    0. 26229.     0.  4913.]\n",
      " [    0. 28309.     0.  1442.]]\n",
      "Non quantized model 2 with quantized weights and biases and quantized input\n",
      "[[    0.  6704. 16489. 17008.]\n",
      " [    0.  9465.  7706. 12192.]\n",
      " [    0. 26229.     0.  4913.]\n",
      " [    0. 28309.     0.  1442.]]\n"
     ]
    }
   ],
   "source": [
    "# Assignation of weights and testing mixed model\n",
    "TARGET_IMAGE = 6300 #100\n",
    "OUT_CHANNEL = 22 #20\n",
    "\n",
    "q_idx_original_part1 = [1, 2]\n",
    "q_idx_original_part2 = list(range(3, 9 + 1))\n",
    "q_idx_new_part2 = list(range(3, 9 + 1))\n",
    "# for i, layer in enumerate(q_aware_model.layers):\n",
    "#     # layer.get_weights()\n",
    "#     if len(layer.get_weights()) != 0:\n",
    "#         print(i)\n",
    "q_weights_1 = [q_aware_model.layers[idx].get_weights() for idx in q_idx_original_part1]\n",
    "q_weights_2 = [q_aware_model.layers[idx].get_weights() for idx in q_idx_original_part2]\n",
    "q_weights_1_1 = q_weights_1[1].pop(1)\n",
    "# print(q_weights_1[1][1])\n",
    "# print(q_weights_1_1)\n",
    "\n",
    "key = keys_list[0]\n",
    "kernel = quantized[key][:,:,0]\n",
    "bias = bias_ints\n",
    "nq2_model_part1.layers[1].set_weights([quantized[key]])\n",
    "nq2_model2_part1.layers[1].set_weights([quantized[key], bias])\n",
    "# print(nq2_model_part1.layers[1].variables[0])\n",
    "\n",
    "semi_quantized_input_image = np.round(train_images[TARGET_IMAGE]/input_scale).astype(int)\n",
    "quantized_input_image = np.round(train_images[TARGET_IMAGE]/input_scale).astype(int) + input_zero\n",
    "quantized_and_dequantized_input = input_scale*(quantized_input_image - input_zero)\n",
    "\n",
    "out_nq2 = nq2_model_part1(semi_quantized_input_image[np.newaxis,:])\n",
    "out_nq22 = nq2_model2_part1(semi_quantized_input_image[np.newaxis,:])\n",
    "\n",
    "self_conv = sp.signal.correlate2d(quantized_input_image - input_zero, kernel[:,:,OUT_CHANNEL], mode = \"valid\").astype(int)\n",
    "self_activ = np.maximum(0, self_conv + bias[OUT_CHANNEL])\n",
    "\n",
    "# out_q = q_model_part1(train_images[np.newaxis,TARGET_IMAGE])\n",
    "\n",
    "increment = 4\n",
    "initial_x = 5\n",
    "initial_y = 3\n",
    "sub_pos_x = (initial_x, initial_x + increment)\n",
    "sub_pos_y = (initial_y, initial_y + increment)\n",
    "position = (0, slice(*sub_pos_y), slice(*sub_pos_x), OUT_CHANNEL)\n",
    "print(\"Non quantized model with quantized weights and quantized input\")\n",
    "print(out_nq2[position].numpy())\n",
    "print(\"Self quantized\")\n",
    "print(self_conv[slice(*sub_pos_y), slice(*sub_pos_x)])\n",
    "print(\"Self quantized with biases\")\n",
    "print(self_activ[slice(*sub_pos_y), slice(*sub_pos_x)])\n",
    "print(\"Layers usage\")\n",
    "print(tf.nn.relu(tf.nn.bias_add(out_nq2, bias))[position].numpy())\n",
    "print(\"Non quantized model 2 with quantized weights and biases and quantized input\")\n",
    "print(out_nq22[position].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.06520641 0.13041282 0.13041282]\n",
      " [0.         0.06520641 0.06520641 0.09780961]\n",
      " [0.         0.22822243 0.         0.0326032 ]\n",
      " [0.         0.22822243 0.         0.        ]]\n",
      "(1, 28, 28)\n",
      "[[0.         0.06520641 0.13041282 0.13041282]\n",
      " [0.         0.06520641 0.06520641 0.09780961]\n",
      " [0.         0.22822243 0.         0.0326032 ]\n",
      " [0.         0.22822243 0.         0.        ]]\n",
      "[-0.0326032  0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(activation_scale * np.round(bias_scales_first_layer[OUT_CHANNEL] * out_nq22[position].numpy() / activation_scale))\n",
    "print(train_images[np.newaxis,TARGET_IMAGE].shape)\n",
    "out_pt1 = q_model_part1(train_images[np.newaxis,TARGET_IMAGE])\n",
    "print(out_pt1[position].numpy())\n",
    "print(np.unique(activation_scale * np.round(bias_scales_first_layer * out_nq22.numpy() / activation_scale) - out_pt1.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[7.1383256e-05 6.0325971e-07 1.9709682e-04 1.2281805e-06 9.9949563e-01\n",
      "  1.5581294e-08 1.9709682e-04 1.0379360e-08 3.5062109e-05 1.8437242e-06]], shape=(1, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[7.1383256e-05 6.0325971e-07 1.9709682e-04 1.2281805e-06 9.9949563e-01\n",
      "  1.5581294e-08 1.9709682e-04 1.0379360e-08 3.5062109e-05 1.8437242e-06]], shape=(1, 10), dtype=float32)\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "# Two part model output\n",
    "TARGET_IMAGE = 6300 #100\n",
    "OUT_CHANNEL = 22 #20\n",
    "\n",
    "semi_quantized_input_image = np.round(train_images[TARGET_IMAGE]/input_scale).astype(int)\n",
    "out_nq2 = nq2_model_part1(semi_quantized_input_image[np.newaxis,:])\n",
    "out_nq2 = tf.nn.relu(tf.nn.bias_add(out_nq2, bias))\n",
    "out_nq2 = activation_scale * np.round(bias_scales_first_layer * out_nq2 / activation_scale)\n",
    "out_q4 = q_model_part2(out_nq2)\n",
    "\n",
    "out_test = q_aware_model(train_images[np.newaxis,TARGET_IMAGE])\n",
    "\n",
    "print(out_q4)\n",
    "print(out_test)\n",
    "print(np.unique(out_test - out_q4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_split_models_generator(q_aware_model : tf.keras.Model, quantized_weights: OrderedDict, keys_list: List):\n",
    "    r\"\"\" Mixed model generation\n",
    "    -\n",
    "    Generates first model as non quantized and second model as quantized.\n",
    "    - First\n",
    "    - Second \"\"\"\n",
    "    INDEX_FIRST_LAYER_KEY_LIST = 1\n",
    "    INDEX_FIRST_CONV_ORIGINAL_MODEL = 2\n",
    "    INDEX_FIRST_CONV_PT1_MODEL = 1\n",
    "    INDEX_QUANTIZE_LAYER_PT2_MODEL = 1\n",
    "    PT2_LENGTH = len(q_aware_model.layers) - 3\n",
    "    START_INDEX_ORIGINAL_MODEL_PT2 = 3\n",
    "    START_INDEX_PT2_MODEL = 2\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape = (28, 28, 1))\n",
    "    conv_1 = tf.keras.layers.Conv2D(32, 5, use_bias = False, activation = None)(input_layer)\n",
    "\n",
    "    input_layer_2 = tf.keras.layers.Input(shape = (24, 24, 32))\n",
    "    pool_1 = tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2)(input_layer_2)\n",
    "    conv_2 = tf.keras.layers.Conv2D(64, 5, use_bias = True, activation = 'relu')(pool_1)\n",
    "    pool_2 = tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2)(conv_2)\n",
    "    conv_3 = tf.keras.layers.Conv2D(96, 3, use_bias = True, activation = 'relu')(pool_2)\n",
    "    pool_3 = tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2)(conv_3)\n",
    "    flat_1 = tf.keras.layers.Flatten()(pool_3)\n",
    "    dense_out = tf.keras.layers.Dense(10, activation = 'softmax', name = \"dense_last\")(flat_1)\n",
    "\n",
    "    # First model: non-quantized with quantized weights\n",
    "    nq_model_part1 = tf.keras.models.Model(inputs = input_layer, outputs = conv_1)\n",
    "    # Second model: quantized\n",
    "    nq_model_part2 = tf.keras.models.Model(inputs = input_layer_2, outputs = dense_out)\n",
    "    q_model_part2 = tfmot.quantization.keras.quantize_model(nq_model_part2)\n",
    "\n",
    "    # Assignation of values for the part 1 model\n",
    "    key = keys_list[INDEX_FIRST_LAYER_KEY_LIST]\n",
    "    nq_model_part1.layers[INDEX_FIRST_CONV_PT1_MODEL].set_weights([quantized_weights[key]])\n",
    "    \n",
    "    # Assignation of max and min values for quantization layer for part 2 model\n",
    "    indexes_original_part2 = list(range(START_INDEX_ORIGINAL_MODEL_PT2, START_INDEX_ORIGINAL_MODEL_PT2 + PT2_LENGTH))\n",
    "    indexes_new_part2 = list(range(START_INDEX_PT2_MODEL, START_INDEX_PT2_MODEL + PT2_LENGTH))\n",
    "    quantize_layer_max_min = q_aware_model.layers[INDEX_FIRST_CONV_ORIGINAL_MODEL].get_weights()[-2:]\n",
    "    quantize_layer_max_min.append(-1)\n",
    "    q_model_part2.layers[INDEX_QUANTIZE_LAYER_PT2_MODEL].set_weights(quantize_layer_max_min)\n",
    "    # Assignation of the rest of the values for the rest of the part 2 model\n",
    "    weights_part2_model = [q_aware_model.layers[idx].get_weights() for idx in indexes_original_part2]\n",
    "    for i, idx in enumerate(indexes_new_part2):\n",
    "        q_model_part2.layers[idx].set_weights(weights_part2_model[i])\n",
    "\n",
    "    q_model_part2.compile(optimizer = 'adam', \n",
    "        loss = 'sparse_categorical_crossentropy', \n",
    "        metrics = ['accuracy'])\n",
    "\n",
    "    return nq_model_part1, q_model_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out part 2\n",
      "tf.Tensor(\n",
      "[[9.9998820e-01 7.6681560e-12 3.3928120e-06 2.9688740e-10 1.0043667e-09\n",
      "  7.6681560e-12 8.4632084e-06 7.6681560e-12 6.9175248e-09 7.6681569e-12]], shape=(1, 10), dtype=float32)\n",
      "Out test\n",
      "tf.Tensor(\n",
      "[[9.9998820e-01 7.6681560e-12 3.3928120e-06 2.9688740e-10 1.0043667e-09\n",
      "  7.6681560e-12 8.4632084e-06 7.6681560e-12 6.9175248e-09 7.6681569e-12]], shape=(1, 10), dtype=float32)\n",
      "Unique Differences [0.]\n"
     ]
    }
   ],
   "source": [
    "# Two part model output\n",
    "TARGET_IMAGE = 1 #100\n",
    "\n",
    "model_pt1_nq, model_pt2_q = mix_split_models_generator(q_aware_model, quantized_weights, new_keys_list)\n",
    "\n",
    "semi_quantized_input_image = np.round(train_images[TARGET_IMAGE]/output_scales[new_keys_list[0]]).astype(int)\n",
    "out_pt1 = model_pt1_nq(semi_quantized_input_image[np.newaxis,:])\n",
    "out_pt1 = tf.nn.relu(tf.nn.bias_add(out_pt1, bias))\n",
    "out_pt1 = activation_scale * np.round(bias_scales_first_layer * out_pt1 / activation_scale)\n",
    "out_pt2 = q_model_part2(out_pt1)\n",
    "\n",
    "out_test = q_aware_model(train_images[np.newaxis,TARGET_IMAGE])\n",
    "\n",
    "print(\"Out part 2\")\n",
    "print(out_pt2)\n",
    "print(\"Out test\")\n",
    "print(out_test)\n",
    "print(\"Unique Differences\", np.unique(out_test - out_pt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "182375.0\n",
      "-198187.0\n",
      "313/313 [==============================] - 2s 5ms/step\n",
      "313/313 [==============================] - 2s 6ms/step\n",
      "[1.6090614e-04 9.9974769e-01 2.5010963e-06 1.7836649e-07 5.0920039e-06\n",
      " 4.9320692e-10 4.6002378e-06 4.9320692e-10 7.9033991e-05 3.0688772e-09]\n",
      "[1.6090614e-04 9.9974769e-01 2.5010963e-06 1.7836649e-07 5.0920039e-06\n",
      " 4.9320692e-10 4.6002378e-06 4.9320692e-10 7.9033991e-05 3.0688772e-09]\n",
      "All set difference 0.05063927\n",
      "Quantity differences 7905\n",
      "Target difference 0.0\n",
      "Location (array([7903, 7904], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "LAYER = 1\n",
    "TARGET = 7904\n",
    "\n",
    "key = new_keys_list[LAYER]\n",
    "semi_quantized_test_images = np.round(test_images/output_scales[new_keys_list[0]]).astype(int)\n",
    "out_test_part1 = model_pt1_nq.predict(semi_quantized_test_images)\n",
    "print(np.max(out_test_part1))\n",
    "print(np.min(out_test_part1))\n",
    "dequantized_out_test_part1 = output_scales[key] * np.round(bias_scales[key] * tf.nn.relu(tf.nn.bias_add(out_test_part1, quantized_bias[key])) / output_scales[key])\n",
    "\n",
    "out_test_part2 = model_pt2_q.predict(dequantized_out_test_part1)\n",
    "out_test = q_aware_model.predict(test_images)\n",
    "print(out_test_part2[TARGET])\n",
    "print(out_test[TARGET])\n",
    "diff = np.unique(out_test_part2 - out_test)\n",
    "m = np.maximum(np.abs(np.max(diff)), np.abs(np.min(diff)))\n",
    "# print(diff)\n",
    "print(\"All set difference\", m)\n",
    "print(\"Quantity differences\", len(diff))\n",
    "print(\"Target difference\", np.max(np.unique(out_test_part2[TARGET] - out_test[TARGET])))\n",
    "eps = 1e-2\n",
    "print(\"Location\", np.where(np.abs(diff -np.max(diff)) < eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.2892 - accuracy: 0.9116\n",
      "Part 2 test accuracy :  91.16%\n",
      "Part 2 test loss:  0.2891831398010254\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2891 - accuracy: 0.9115\n",
      "Q Aware model test accuracy :  91.15%\n",
      "Q Aware model test loss:  0.2890910804271698\n"
     ]
    }
   ],
   "source": [
    "part2_test_loss, part2_test_acc = model_pt2_q.evaluate(dequantized_out_test_part1, test_labels)\n",
    "print('Part 2 test accuracy : ', \"{:0.2%}\".format(part2_test_acc))\n",
    "print('Part 2 test loss: ', part2_test_loss)\n",
    "q_aware_test_loss, q_aware_test_acc = q_aware_model.evaluate(test_images, test_labels)\n",
    "print('Q Aware model test accuracy : ', \"{:0.2%}\".format(q_aware_test_acc))\n",
    "print('Q Aware model test loss: ', q_aware_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2291356\n"
     ]
    }
   ],
   "source": [
    "x = -194204\n",
    "y = n_bit_flipper(x, bit_width = 32, bit_pos = 21)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "float32\n",
      "input scale 0.003921569 float32\n",
      "activation scale 0.032603204 float32\n",
      "after quantization input <dtype: 'float32'>\n",
      "kernel scales float32\n",
      "kernel quantized float32\n",
      "after convolution <dtype: 'float32'>\n",
      "after bias <dtype: 'float32'>\n",
      "after relu <dtype: 'float32'>\n",
      "division <dtype: 'float32'>\n",
      "dequantized float32\n",
      "input division [0.]\n",
      "Difference after input quantize layer [0.] [7840000]\n",
      "Difference after convolution layer [-0.03260326 -0.0326032   0.          0.03260314  0.0326032 ] [        1      2636 184312977        34      4352]\n",
      "Difference after convolution layer ints [-0.03260326 -0.0326032   0.          0.03260314  0.0326032 ] [        1      2640 184312972        34      4353]\n"
     ]
    }
   ],
   "source": [
    "# test_images[:,:,:,np.newaxis].shape\n",
    "# Input layer\n",
    "a = q_aware_model.layers[0](test_images[:,:,:,np.newaxis])\n",
    "# Quantize layer\n",
    "b = q_aware_model.layers[1](a)\n",
    "# Convolutional layer\n",
    "c = q_aware_model.layers[2](b)\n",
    "print(b.shape)\n",
    "pos = 0, slice(10,15), slice(10,15), 0\n",
    "\n",
    "input_scale = ((input_max - input_min) / (2**BIT_WIDTH - 1)).astype(np.float32)\n",
    "# kernel_scales = kernel_max / (2**(BIT_WIDTH - 1) - 1)\n",
    "# bias_scales_first_layer = input_scale * kernel_scales\n",
    "print((input_scale * kernel_scales).dtype)\n",
    "activation_scale = ((activation_max - activation_min) / (2**BIT_WIDTH - 1)).astype(np.float32)\n",
    "\n",
    "print(\"input scale\", input_scale, input_scale.dtype)\n",
    "print(\"activation scale\", activation_scale, activation_scale.dtype)\n",
    "print(\"after quantization input\", b.dtype)\n",
    "print(\"kernel scales\", kernel_scales.dtype)\n",
    "print(\"kernel quantized\", (kernel_scales*quantized[keys_list[0]]).dtype)\n",
    "b_2 = tf.nn.conv2d(b, kernel_scales*quantized[keys_list[0]], strides = 1, padding = \"VALID\")\n",
    "print(\"after convolution\", b_2.dtype)\n",
    "b_2_bias = tf.nn.bias_add(b_2, (input_scale*kernel_scales).astype(np.float32)*bias_ints)\n",
    "print(\"after bias\", b_2_bias.dtype)\n",
    "b_2_relu = tf.nn.relu(b_2_bias)\n",
    "print(\"after relu\", b_2_relu.dtype)\n",
    "print(\"division\", (b_2_relu / activation_scale).dtype)\n",
    "deq_out_test_part1 = activation_scale * np.round(b_2_relu / activation_scale)\n",
    "print(\"dequantized\", deq_out_test_part1.dtype)\n",
    "\n",
    "print(\"input division\", np.unique(semi_quantized_input_image - np.round((train_images[:,:,:,np.newaxis]/input_scale).astype(np.float32))))\n",
    "semi_quantized_test_images = np.round(test_images[:,:,:,np.newaxis]/input_scale).astype(int)\n",
    "dequant = input_scale * semi_quantized_test_images\n",
    "\n",
    "b_2_quant = tf.nn.conv2d(semi_quantized_test_images, quantized[keys_list[0]], strides = 1, padding = \"VALID\")\n",
    "b_2_quant_bias = tf.nn.bias_add(b_2_quant, bias_ints) \n",
    "b_2_quant_relu = tf.nn.relu(b_2_quant_bias)\n",
    "b_2_dequant = activation_scale * np.round( input_scale * kernel_scales* tf.cast(b_2_quant_relu, np.float32) / activation_scale)\n",
    "\n",
    "# print(test_images[pos[:-1]])\n",
    "# print(b[pos])\n",
    "diff_quantized_input, quantized_input_counts = np.unique(dequant - b, return_counts = True)\n",
    "diff_dequantized_output, dequantized_output_counts = np.unique(c - deq_out_test_part1, return_counts = True)\n",
    "diff_dequant_ints_output, dequant_ints_output_counts = np.unique(c - b_2_dequant, return_counts = True)\n",
    "print(\"Difference after input quantize layer\", diff_quantized_input, quantized_input_counts)\n",
    "print(\"Difference after convolution layer\", diff_dequantized_output, dequantized_output_counts)\n",
    "print(\"Difference after convolution layer ints\", diff_dequant_ints_output, dequant_ints_output_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference after convolution layer [-0.03260326 -0.0326032   0.          0.03260314  0.0326032 ] [        1      2636 184312977        34      4352]\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference after convolution layer\", diff_dequantized_output, dequantized_output_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   4   0]\n",
      " [  0   0   2   0  11]\n",
      " [  0   3   0   0 115]\n",
      " [  3   0   0  89 139]\n",
      " [  0   0  98 136 110]]\n",
      "tf.Tensor(\n",
      "[[3216.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]\n",
      " [   0.    0.  445.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(semi_quantized_test_images[pos])\n",
    "print(tf.cast(b_2_quant_relu, np.float32)[pos])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_master_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27b772b08f604b6b43e5a3a606000b08c03a565bb0332f867b1c8863c689a183"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
