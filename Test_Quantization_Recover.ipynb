{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import ast\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH_Q_AWARE = \"./model/\" + \"model_qware_final_01\"\n",
    "\n",
    "q_aware_model : tf.keras.Model\n",
    "with tfmot.quantization.keras.quantize_scope():\n",
    "    q_aware_model = tf.keras.models.load_model(LOAD_PATH_Q_AWARE)\n",
    "\n",
    "\n",
    "def bit_flipper(value : int, bit_pos : int) -> int:\n",
    "    \"\"\" Random bit flipper \n",
    "    -\n",
    "    Obtains a value and bit position and flips it.\n",
    "    - All values are in 8 bits, MSB have higher probability of getting flipped\n",
    "    - It is assumed value is a signed 8 bit number \"\"\"\n",
    "    # Negative 2 Complement conversion\n",
    "    if value < 0:\n",
    "        value = (-value ^ 0xFF) + 1\n",
    "    flip_mask = 1 << bit_pos\n",
    "    flipped_value = value ^ flip_mask\n",
    "    # Negative back conversion 2 Complement\n",
    "    if flipped_value >= 128:\n",
    "        flipped_value = -((flipped_value ^ 0xFF) + 1)\n",
    "    return flipped_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2891 - accuracy: 0.9115\n",
      "Test accuracy :  91.15%\n"
     ]
    }
   ],
   "source": [
    "q_aware_test_loss, q_aware_test_acc = q_aware_model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy : ', \"{:0.2%}\".format(q_aware_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d/kernel (5, 5, 1, 32)\n",
      "[[ -34.    9.   41.  -66. -127.]\n",
      " [ -70.    8.   57.   27.  -73.]\n",
      " [   5.   39.  -30.  -17.   33.]\n",
      " [  47.  -23.   -1.  -23.   -1.]\n",
      " [  39.   16.  -19.    4.   12.]]\n",
      "conv2d_1/kernel (5, 5, 32, 64)\n",
      "[[-18. -40. -32.   4.  -3.]\n",
      " [  5. -28. -42.  12.  20.]\n",
      " [ 47. -42. -14. -11.  32.]\n",
      " [ 41. -15.   0. -15.  17.]\n",
      " [ 35. -10.  -4.  17.  25.]]\n",
      "conv2d_2/kernel (3, 3, 64, 96)\n",
      "[[ 69.  51.  -2.]\n",
      " [  2. -98. -63.]\n",
      " [ 67.   5. -44.]]\n",
      "dense_last/kernel (96, 10)\n",
      "[  4.   1.  -6.   6.  -1. -10. -18.  20. -18.  -8.  -1.  -6. -30. -13.\n",
      "  -8.  11.  13.   5. -35. -12. -24. -18.   8.   8.  -4. -27.  18.  15.\n",
      " -46. -23.  21. -42.  19.  12.  12.  -1.   9.  -8.   1.  18.  -6.  13.\n",
      "  -2.  13.   8. -10.  12.  27.   5.  -1.  -5.  -5. -11.  13. -29. -25.\n",
      "  -8.   4.  19.   4. -19. -26.  32.  15. -35. -12.   0.  31.  -8. -25.\n",
      "  10.  21.  14.  14.  -9.   7. -24. -23. -15.  18.   6.  27.  26.  13.\n",
      "   9.  34.   6. -25. -23. -14. -42. -17.  -2.   7. -18.   9.]\n"
     ]
    }
   ],
   "source": [
    "bit_width = 8\n",
    "quantized_and_dequantized = OrderedDict()\n",
    "quantized = OrderedDict()\n",
    "layer_index_list = []\n",
    "keys_list = []\n",
    "\n",
    "layer : tfmot.quantization.keras.QuantizeWrapperV2\n",
    "for i, layer in enumerate(q_aware_model.layers):\n",
    "    quantizer : tfmot.quantization.keras.quantizers.Quantizer\n",
    "    weight : tf.Variable\n",
    "    if hasattr(layer, '_weight_vars'):\n",
    "        for weight, quantizer, quantizer_vars in layer._weight_vars:\n",
    "            min_var = quantizer_vars['min_var']\n",
    "            max_var = quantizer_vars['max_var']\n",
    "\n",
    "            key = weight.name[:-2]\n",
    "            layer_index_list.append(i)\n",
    "            keys_list.append(key)\n",
    "            quantized_and_dequantized[key] = quantizer(inputs = weight, training = False, weights = quantizer_vars)\n",
    "            quantized[key] = np.round(quantized_and_dequantized[key] / max_var * (2**(bit_width-1)-1))\n",
    "\n",
    "for key in quantized:\n",
    "    # print(\"Fake Quantized\")\n",
    "    print(key, quantized[key].shape)\n",
    "    if \"dense\" not in key:\n",
    "        # print(quantized_and_dequantized[key][:,:,0,0])\n",
    "        print(quantized[key][:,:,0,0])\n",
    "    else:\n",
    "        # print(quantized_and_dequantized[key][:,0])\n",
    "        print(quantized[key][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[  22.    1.   73.  102.   -7.]\n",
      " [  56.   50.   37.   77.   68.]\n",
      " [ -75.   21.   -5.   69.   19.]\n",
      " [ -38.  -30.  -74.   52.   -7.]\n",
      " [ -66.  -65. -127.   23.   23.]]\n"
     ]
    }
   ],
   "source": [
    "conv_idx = 0 \n",
    "key = keys_list[conv_idx]\n",
    "print(type(quantized[key][:,:,0,29]))\n",
    "print(quantized[key][:,:,0,29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 0, 28)\n",
      "6\n",
      "-12.0\n",
      "-12.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Performance.csv')\n",
    "position_disrupted_list = []\n",
    "bit_flipped_list = []\n",
    "for d in df['position_disrupted']:\n",
    "    try:\n",
    "        position_disrupted_list.append(ast.literal_eval(d))\n",
    "    except:\n",
    "        position_disrupted_list.append(tuple())\n",
    "for d in df['bit_disrupted']:\n",
    "    try:\n",
    "        bit_flipped_list.append(int(d))\n",
    "    except:\n",
    "        bit_flipped_list.append(-1)\n",
    "\n",
    "BIT_WIDTH = 8\n",
    "out_list = []\n",
    "conv_idx = 0 \n",
    "key = keys_list[conv_idx]\n",
    "layer_index = layer_index_list[conv_idx]\n",
    "T_VARIABLES_KERNEL_INDEX = 0\n",
    "\n",
    "pos = 2013\n",
    "print(position_disrupted_list[pos])\n",
    "print(bit_flipped_list[pos])\n",
    "print(df['quantized_value'][pos])\n",
    "print(quantized[key][position_disrupted_list[pos]])\n",
    "for i, position in enumerate(position_disrupted_list):\n",
    "    entry = {}\n",
    "    entry['data'] = position\n",
    "    if len(position) > 1:\n",
    "        m_vars = {variable.name: variable for i, variable in enumerate(q_aware_model.layers[layer_index].non_trainable_variables) if keys_list[conv_idx] in variable.name}\n",
    "        min_key = list(key for key in m_vars if \"min\" in key)[0]\n",
    "        max_key = list(key for key in m_vars if \"max\" in key)[0]\n",
    "        min_var = m_vars[min_key]\n",
    "        max_var = m_vars[max_key]\n",
    "        entry['min_var'] = min_var[position[-1]].numpy()\n",
    "        entry['max_var'] = max_var[position[-1]].numpy()\n",
    "        entry['original_weight_value'] = q_aware_model.layers[layer_index].trainable_variables[T_VARIABLES_KERNEL_INDEX][position].numpy()\n",
    "        entry['quantized_value'] = quantized[key][position]\n",
    "        entry['flipped_quantized_value'] = bit_flipper(int(quantized[key][position]), bit_flipped_list[i])\n",
    "        entry['flipped_weight_value'] = entry['flipped_quantized_value'] * max_var.numpy()[position[-1]] / (2**(BIT_WIDTH - 1) - 1)\n",
    "        # Laplacian calculation\n",
    "        kernel = q_aware_model.layers[layer_index].trainable_variables[T_VARIABLES_KERNEL_INDEX][:,:,position[2],position[-1]].numpy() # Passed as copy\n",
    "        original_laplacian = sp.ndimage.laplace(kernel)\n",
    "        kernel[position[:2]] = entry['flipped_weight_value']\n",
    "        new_laplacian = sp.ndimage.laplace(kernel)\n",
    "        entry['original_laplacian'] = original_laplacian[position[:2]]\n",
    "        entry['modified_laplacian'] = new_laplacian[position[:2]]\n",
    "        # When assigning a whole array it is passed as reference\n",
    "        # Important to avoid modifying the original values\n",
    "        int_kernel = np.copy(quantized[key][:,:,position[2],position[-1]])\n",
    "        original_int_laplacian = sp.ndimage.laplace(int_kernel)\n",
    "        int_kernel[position[:2]] = entry['flipped_quantized_value']\n",
    "        new_int_laplacian = sp.ndimage.laplace(int_kernel)    \n",
    "        entry['original_int_laplacian'] = original_int_laplacian[position[:2]]\n",
    "        entry['modified_int_laplacian'] = new_int_laplacian[position[:2]]\n",
    "    else:\n",
    "        entry['min_var'] = None\n",
    "        entry['max_var'] = None\n",
    "        entry['original_weight_value'] = None\n",
    "        entry['quantized_value'] = None\n",
    "        entry['flipped_quantized_value'] = None\n",
    "        entry['flipped_weight_value'] = None\n",
    "        entry['original_laplacian'] = None\n",
    "        entry['modified_laplacian'] = None\n",
    "        entry['original_int_laplacian'] = None\n",
    "        entry['modified_int_laplacian'] = None\n",
    "    out_list.append(entry)\n",
    "out_df = pd.DataFrame(out_list)\n",
    "out_df.to_csv('Max_Min.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_master_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27b772b08f604b6b43e5a3a606000b08c03a565bb0332f867b1c8863c689a183"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
