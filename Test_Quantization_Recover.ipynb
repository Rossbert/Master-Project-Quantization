{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import ast\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH_Q_AWARE = \"./model/\" + \"model_q_aware_final_01\"\n",
    "LOAD_TFLITE_PATH = \"./model/\" + 'tflite_final_01.tflite'\n",
    "# Load Q Aware model\n",
    "q_aware_model : tf.keras.Model\n",
    "with tfmot.quantization.keras.quantize_scope():\n",
    "    q_aware_model = tf.keras.models.load_model(LOAD_PATH_Q_AWARE)\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(LOAD_TFLITE_PATH)\n",
    "\n",
    "def bit_flipper(value : int, bit_pos : int) -> int:\n",
    "    \"\"\" Random bit flipper \n",
    "    -\n",
    "    Obtains a value and bit position and flips it.\n",
    "    - All values are in 8 bits, MSB have higher probability of getting flipped\n",
    "    - It is assumed value is a signed 8 bit number \"\"\"\n",
    "    # Negative 2 Complement conversion\n",
    "    if value < 0:\n",
    "        value = (-value ^ 0xFF) + 1\n",
    "    flip_mask = 1 << bit_pos\n",
    "    flipped_value = value ^ flip_mask\n",
    "    # Negative back conversion 2 Complement\n",
    "    if flipped_value >= 128:\n",
    "        flipped_value = -((flipped_value ^ 0xFF) + 1)\n",
    "    return flipped_value\n",
    "\n",
    "def evaluate_model(interpreter: tf.lite.Interpreter) -> Tuple[float, float]:\n",
    "    \"\"\" Evaluate TFLite Model:\n",
    "    -\n",
    "    Receives the interpreter and returns a tuple of loss and accuracy.\n",
    "    \"\"\"\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    predictions = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis = 0).astype(np.float32)\n",
    "        test_image = np.expand_dims(test_image, axis = 3).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        predictions.append(np.copy(output()[0]))\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    predictions = np.array(predictions)\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()(test_labels, predictions)\n",
    "\n",
    "    loss = scce.numpy()\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy of both models\n",
    "q_aware_test_loss, q_aware_test_acc = q_aware_model.evaluate(test_images, test_labels)\n",
    "print('Q Aware model test accuracy: ', \"{:0.2%}\".format(q_aware_test_acc))\n",
    "print('Q Aware model test loss: ', q_aware_test_loss)\n",
    "interpreter.allocate_tensors()\n",
    "tflite_loss, tflite_accuracy = evaluate_model(interpreter)\n",
    "print('TFLite model test accuracy: ', \"{:0.2%}\".format(tflite_accuracy))\n",
    "print('TFLite model test loss: ', tflite_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIT_WIDTH = 8\n",
    "quantized_and_dequantized = OrderedDict()\n",
    "quantized = OrderedDict()\n",
    "layer_index_list = []\n",
    "keys_list = []\n",
    "\n",
    "layer : tfmot.quantization.keras.QuantizeWrapperV2\n",
    "for i, layer in enumerate(q_aware_model.layers):\n",
    "    quantizer : tfmot.quantization.keras.quantizers.Quantizer\n",
    "    weight : tf.Variable\n",
    "    if hasattr(layer, '_weight_vars'):\n",
    "        for weight, quantizer, quantizer_vars in layer._weight_vars:\n",
    "            min_var = quantizer_vars['min_var']\n",
    "            max_var = quantizer_vars['max_var']\n",
    "\n",
    "            key = weight.name[:-2]\n",
    "            layer_index_list.append(i)\n",
    "            keys_list.append(key)\n",
    "            quantized_and_dequantized[key] = quantizer(inputs = weight, training = False, weights = quantizer_vars)\n",
    "            quantized[key] = np.round(quantized_and_dequantized[key] / max_var * (2**(BIT_WIDTH-1)-1))\n",
    "\n",
    "for key in quantized:\n",
    "    # print(\"Fake Quantized\")\n",
    "    print(key, quantized[key].shape)\n",
    "    if \"dense\" not in key:\n",
    "        # print(quantized_and_dequantized[key][:,:,0,0])\n",
    "        print(quantized[key][:,:,0,0])\n",
    "    else:\n",
    "        # print(quantized_and_dequantized[key][:,0])\n",
    "        print(quantized[key][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_idx = 0 \n",
    "key = keys_list[kernel_idx]\n",
    "print(type(quantized[key][:,:,0,29]))\n",
    "print(quantized[key][:,:,0,29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Performance.csv')\n",
    "layer_affected_list = []\n",
    "kernel_index_list = []\n",
    "layer_affected_index_list = []\n",
    "position_disrupted_list = []\n",
    "bit_flipped_list = []\n",
    "row : pd.Series\n",
    "for index, row in df.iterrows():\n",
    "    layer_affected_list.append(row['layer_affected'])\n",
    "    try:\n",
    "        kernel_index_list.append(int(row['kernel_index']))\n",
    "    except:\n",
    "        kernel_index_list.append(-1)\n",
    "    try:\n",
    "        layer_affected_index_list.append(int(row['layer_affected_index']))\n",
    "    except:\n",
    "        layer_affected_index_list.append(-1)\n",
    "    try:\n",
    "        position_disrupted_list.append(ast.literal_eval(row['position_disrupted']))\n",
    "    except:\n",
    "        position_disrupted_list.append(tuple())\n",
    "    try:\n",
    "        bit_flipped_list.append(int(row['bit_disrupted']))\n",
    "    except:\n",
    "        bit_flipped_list.append(-1)\n",
    "\n",
    "BIT_WIDTH = 8\n",
    "T_VARIABLES_KERNEL_INDEX = 0\n",
    "out_list = []\n",
    "\n",
    "pos = 2013\n",
    "print(position_disrupted_list[pos])\n",
    "print(bit_flipped_list[pos])\n",
    "print(df['quantized_value'][pos])\n",
    "print(quantized[key][position_disrupted_list[pos]])\n",
    "for i, key in enumerate(layer_affected_list):\n",
    "    print(\"Index\", i)\n",
    "    entry = {}\n",
    "    position = position_disrupted_list[i]\n",
    "    if len(position) > 1:\n",
    "        q_aware_copy : tf.keras.Model\n",
    "        # Load Q Aware model copy\n",
    "        with tfmot.quantization.keras.quantize_scope():\n",
    "            q_aware_copy = tf.keras.models.load_model(LOAD_PATH_Q_AWARE)\n",
    "\n",
    "        if \"dense\" not in key:\n",
    "            # It is a convolutional layer\n",
    "            kernel_row = position[0]\n",
    "            kernel_column = position[1]\n",
    "            in_channel = position[2]\n",
    "            out_channel = position[3]\n",
    "            kernel_position = (slice(None), slice(None), in_channel, out_channel)\n",
    "            value_position = (kernel_row, kernel_column)\n",
    "        else:\n",
    "            # It is a fully connected layer\n",
    "            kernel_row = None\n",
    "            kernel_column = None\n",
    "            in_channel = position[0]\n",
    "            out_channel = position[1]\n",
    "            kernel_position = (slice(None), slice(None)) # This slice takes the whole densely connected kernel\n",
    "            value_position = (in_channel, out_channel)\n",
    "\n",
    "        kernel_idx = kernel_index_list[i]\n",
    "        layer_index = layer_index_list[kernel_idx]\n",
    "        m_vars = {variable.name: variable for i, variable in enumerate(q_aware_model.layers[layer_index].non_trainable_variables) if keys_list[kernel_idx] in variable.name}\n",
    "        min_key = list(key for key in m_vars if \"min\" in key)[0]\n",
    "        max_key = list(key for key in m_vars if \"max\" in key)[0]\n",
    "        if \"dense\" not in key:\n",
    "            # Convolutional layers max is divided per channels\n",
    "            min_var = m_vars[min_key][out_channel]\n",
    "            max_var = m_vars[max_key][out_channel]\n",
    "        else:\n",
    "            # Fully connected layer has only 1 max value for the kernel\n",
    "            min_var = m_vars[min_key]\n",
    "            max_var = m_vars[max_key]\n",
    "\n",
    "        flipped_int_kernel_value = bit_flipper(int(quantized[key][position]), bit_flipped_list[i])\n",
    "        flipped_float_kernel_val = flipped_int_kernel_value * max_var.numpy() / (2**(BIT_WIDTH - 1) - 1)\n",
    "        full_kernel = q_aware_copy.layers[layer_index].trainable_variables[T_VARIABLES_KERNEL_INDEX].numpy()\n",
    "        update_kernel = np.copy(full_kernel)\n",
    "        update_kernel[position] = flipped_float_kernel_val\n",
    "        q_aware_copy.layers[layer_index].trainable_variables[T_VARIABLES_KERNEL_INDEX].assign(update_kernel)\n",
    "        # Laplacian calculation\n",
    "        kernel = full_kernel[kernel_position]\n",
    "        original_laplacian = sp.ndimage.laplace(kernel)\n",
    "        new_laplacian = sp.ndimage.laplace(update_kernel[kernel_position])\n",
    "        int_kernel = np.copy(quantized[key][kernel_position]) # Important to avoid modifying the original values\n",
    "        original_int_laplacian = sp.ndimage.laplace(int_kernel)\n",
    "        int_kernel[value_position] = flipped_int_kernel_value\n",
    "        new_int_laplacian = sp.ndimage.laplace(int_kernel)    \n",
    "        \n",
    "        # Conversion of new model to TF Lite model\n",
    "        new_converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_copy)\n",
    "        new_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        new_tflite_model = new_converter.convert()\n",
    "        new_interpreter = tf.lite.Interpreter(model_content = new_tflite_model)\n",
    "\n",
    "        # Check new accuracy\n",
    "        q_copy_test_loss, q_copy_test_acc = q_aware_copy.evaluate(test_images, test_labels, verbose = 0)\n",
    "        new_interpreter.allocate_tensors()\n",
    "        new_tflite_loss, new_tflite_accuracy = evaluate_model(new_interpreter)\n",
    "\n",
    "        entry['position'] = position\n",
    "        entry['min_var'] = min_var.numpy()\n",
    "        entry['max_var'] = max_var.numpy()\n",
    "        entry['original_weight_value'] = full_kernel[position]\n",
    "        entry['quantized_value'] = quantized[key][position]\n",
    "        entry['bit_disrupted'] = bit_flipped_list[i]\n",
    "        entry['flipped_quantized_value'] = flipped_int_kernel_value\n",
    "        entry['flipped_weight_value'] = flipped_float_kernel_val\n",
    "        entry['q_aware_accuracy'] = q_copy_test_acc\n",
    "        entry['tflite_accuracy'] = new_tflite_accuracy\n",
    "        entry['q_aware_acc_degradation'] = q_copy_test_acc - q_aware_test_acc\n",
    "        entry['tflite_acc_degradation'] = new_tflite_accuracy - tflite_accuracy\n",
    "        entry['q_aware_loss'] = q_copy_test_loss\n",
    "        entry['tflite_loss'] = new_tflite_loss\n",
    "        entry['original_laplacian'] = original_laplacian[value_position]\n",
    "        entry['modified_laplacian'] = new_laplacian[value_position]\n",
    "        entry['original_int_laplacian'] = original_int_laplacian[value_position]\n",
    "        entry['modified_int_laplacian'] = new_int_laplacian[value_position]\n",
    "        entry['abs_laplacian_diff'] = np.abs(entry['original_laplacian'] - entry['modified_laplacian'])\n",
    "        entry['abs_int_laplacian_diff'] = np.abs(entry['original_int_laplacian'] - entry['modified_int_laplacian'])\n",
    "    else:\n",
    "        entry['position'] = None\n",
    "        entry['min_var'] = None\n",
    "        entry['max_var'] = None\n",
    "        entry['original_weight_value'] = None\n",
    "        entry['quantized_value'] = None\n",
    "        entry['bit_disrupted'] = None\n",
    "        entry['flipped_quantized_value'] = None\n",
    "        entry['flipped_weight_value'] = None\n",
    "        entry['q_aware_accuracy'] = q_aware_test_acc\n",
    "        entry['tflite_accuracy'] = tflite_accuracy\n",
    "        entry['q_aware_acc_degradation'] = None\n",
    "        entry['tflite_acc_degradation'] = None\n",
    "        entry['q_aware_loss'] = q_aware_test_loss\n",
    "        entry['tflite_loss'] = tflite_loss\n",
    "        entry['original_laplacian'] = None\n",
    "        entry['modified_laplacian'] = None\n",
    "        entry['original_int_laplacian'] = None\n",
    "        entry['modified_int_laplacian'] = None\n",
    "        entry['abs_laplacian_diff'] = None\n",
    "        entry['abs_int_laplacian_diff'] = None\n",
    "    out_list.append(entry)\n",
    "out_df = pd.DataFrame(out_list)\n",
    "out_df.to_csv('Max_Min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of loss calculation\n",
    "import time\n",
    "q_aware_test_loss, q_aware_test_acc = q_aware_model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy : ', \"{:0.2%}\".format(q_aware_test_acc))\n",
    "print(\"Test accuracy : \", q_aware_test_loss)\n",
    "prediction = q_aware_model.predict(test_images)\n",
    "start_time = time.time()\n",
    "loss_self = np.array([-np.log(prediction[idx][test_labels[idx]]) for idx in range(test_images.shape[0])])\n",
    "print(\"Self calculated loss\")\n",
    "# print(loss_self)\n",
    "print(np.mean(loss_self))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# print(test_labels)\n",
    "# print(prediction)\n",
    "start_time = time.time()\n",
    "loss_1 = tf.keras.losses.sparse_categorical_crossentropy(test_labels, prediction)\n",
    "print(\"Function calculated loss\")\n",
    "# print(loss_1)\n",
    "print(np.mean(loss_1))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()(test_labels, prediction)\n",
    "print(\"Class calculated loss\")\n",
    "print(scce.numpy())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 81\n",
      "Rest of line 9.0,9.2,9.4,9.6,9.8,10.0,10.2,10.4,10.6,10.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "WRITE_FILE_PATH = './Test_file.csv'\n",
    "FLAG = os.path.exists(WRITE_FILE_PATH)\n",
    "\n",
    "with open(WRITE_FILE_PATH, 'r') as file:\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file.seek(file.tell() - 3, os.SEEK_SET)\n",
    "    pos = file.tell()\n",
    "    while file.read(1) != '\\n':\n",
    "        pos -= 1\n",
    "        file.seek(pos, os.SEEK_SET)\n",
    "    cat = ''\n",
    "    while file.read(1) != ',':\n",
    "        pos += 1\n",
    "        file.seek(pos, os.SEEK_SET)\n",
    "        cat += file.read(1)\n",
    "    print(\"Last\", cat)\n",
    "    print(\"Rest of line\", file.readline())\n",
    "    file.close()\n",
    "\n",
    "with open(WRITE_FILE_PATH, 'a') as file:\n",
    "    writer = csv.writer(file, delimiter = ',', lineterminator = '\\n',)\n",
    "    # reader = csv.reader(file, delimiter = ',')\n",
    "    if not FLAG:\n",
    "        writer.writerow([''] + ['a']*1_0)\n",
    "    for i in range(1_0):\n",
    "        row = [i**2] + [i + j*0.2 for j in range(i+1)]\n",
    "        writer.writerow(row)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 32)\n",
      "[[ -34.    9.   41.  -66. -127.]\n",
      " [ -70.    8.   57.   27.  -73.]\n",
      " [   5.   39.  -30.  -17.   33.]\n",
      " [  47.  -23.   -1.  -23.   -1.]\n",
      " [  39.   16.  -19.    4.   12.]]\n",
      "[[-0.11191808  0.02962537  0.13496004 -0.21725276 -0.41804698]\n",
      " [-0.23041959  0.02633367  0.18762738  0.08887613 -0.24029471]\n",
      " [ 0.01645854  0.12837663 -0.09875125 -0.05595904  0.10862638]\n",
      " [ 0.1547103  -0.07570929 -0.00329171 -0.07570929 -0.00329171]\n",
      " [ 0.12837663  0.05266733 -0.06254246  0.01316683  0.0395005 ]]\n",
      "[[-0.11190058  0.02968932  0.13516179 -0.21755771 -0.41798258]\n",
      " [-0.23117375  0.02469435  0.18612708  0.08809777 -0.24172066]\n",
      " [ 0.01567534  0.130002   -0.09754552 -0.05695752  0.10985629]\n",
      " [ 0.15383062 -0.07598737 -0.00296063 -0.07511681 -0.0047392 ]\n",
      " [ 0.12786986  0.0541554  -0.06213173  0.0122421   0.03846057]]\n",
      "0.029690962\n"
     ]
    }
   ],
   "source": [
    "MODELS_DIR = \"./model/\"\n",
    "LOAD_PATH_Q_AWARE = MODELS_DIR + \"model_q_aware_final_01\"\n",
    "LOAD_TFLITE_PATH = MODELS_DIR + 'tflite_final_01.tflite'\n",
    "OUTPUTS_DIR = \"./outputs/\"\n",
    "\n",
    "SAVE_NAME_SHAPE = \"l1_shape.txt\"\n",
    "SAVE_NAME_W_INT = \"l1_weights_int.txt\"\n",
    "SAVE_NAME_W_TRUNC = \"l1_weights_trunc.txt\"\n",
    "SAVE_NAME_W_ORIGINAL = \"l1_weights_original.txt\"\n",
    "SAVE_NAME_B_ORIGINAL = \"l1_bias_original.txt\"\n",
    "\n",
    "SAVE_PATH_SHAPE = OUTPUTS_DIR + SAVE_NAME_SHAPE\n",
    "SAVE_PATH_W_INT = OUTPUTS_DIR + SAVE_NAME_W_INT\n",
    "SAVE_PATH_W_TRUNC = OUTPUTS_DIR + SAVE_NAME_W_TRUNC\n",
    "SAVE_PATH_W_ORIGINAL = OUTPUTS_DIR + SAVE_NAME_W_ORIGINAL\n",
    "SAVE_PATH_B_ORIGINAL = OUTPUTS_DIR + SAVE_NAME_B_ORIGINAL\n",
    "\n",
    "if not os.path.exists(OUTPUTS_DIR):\n",
    "    os.mkdir(OUTPUTS_DIR)\n",
    "\n",
    "# Load Q Aware model\n",
    "q_aware_model : tf.keras.Model\n",
    "with tfmot.quantization.keras.quantize_scope():\n",
    "    q_aware_model = tf.keras.models.load_model(LOAD_PATH_Q_AWARE)\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(LOAD_TFLITE_PATH)\n",
    "\n",
    "# Quantification of values\n",
    "BIT_WIDTH = 8\n",
    "quantized_and_dequantized = OrderedDict()\n",
    "quantized = OrderedDict()\n",
    "layer_index_list = []\n",
    "keys_list = []\n",
    "layers_shapes = []\n",
    "\n",
    "layer : tfmot.quantization.keras.QuantizeWrapperV2\n",
    "for i, layer in enumerate(q_aware_model.layers):\n",
    "    quantizer : tfmot.quantization.keras.quantizers.Quantizer\n",
    "    weight : tf.Variable\n",
    "    if hasattr(layer, '_weight_vars'):\n",
    "        for weight, quantizer, quantizer_vars in layer._weight_vars:\n",
    "            min_var = quantizer_vars['min_var']\n",
    "            max_var = quantizer_vars['max_var']\n",
    "\n",
    "            key = weight.name[:-2]\n",
    "            layer_index_list.append(i)\n",
    "            keys_list.append(key)\n",
    "            layers_shapes.append(weight.numpy().shape)\n",
    "            quantized_and_dequantized[key] = quantizer(inputs = weight, training = False, weights = quantizer_vars)\n",
    "            quantized[key] = np.round(quantized_and_dequantized[key] / max_var * (2**(BIT_WIDTH-1)-1))\n",
    "\n",
    "# First layer\n",
    "LAYER_TARGET = 0\n",
    "T_VARIABLES_KERNEL_INDEX = 0\n",
    "T_VARIABLES_BIAS_INDEX = 1\n",
    "key = keys_list[LAYER_TARGET]\n",
    "kernel_idx = keys_list.index(key)\n",
    "layer_index = layer_index_list[kernel_idx]\n",
    "layer_shape = layers_shapes[LAYER_TARGET]\n",
    "\n",
    "TEST_IN_CHANNEL = 0\n",
    "TEST_OUT_CHANNEL = 0\n",
    "print(layer_shape)\n",
    "print(quantized[key][:,:,0,TEST_OUT_CHANNEL])\n",
    "print(quantized_and_dequantized[key][:,:,TEST_IN_CHANNEL,TEST_OUT_CHANNEL].numpy())\n",
    "print(q_aware_model.layers[layer_index].trainable_variables[T_VARIABLES_KERNEL_INDEX][:,:,TEST_IN_CHANNEL,TEST_OUT_CHANNEL].numpy())\n",
    "print(q_aware_model.layers[layer_index].trainable_variables[T_VARIABLES_BIAS_INDEX][TEST_OUT_CHANNEL].numpy())\n",
    "\n",
    "with open(SAVE_PATH_SHAPE, 'w') as file:\n",
    "    writer = csv.writer(file, delimiter = ' ', lineterminator = '\\n')\n",
    "    writer.writerow(list(layer_shape))\n",
    "with open(SAVE_PATH_W_INT, 'w') as file:\n",
    "    writer = csv.writer(file, delimiter = ' ', lineterminator = '\\n')\n",
    "    for l in range(layer_shape[3]):\n",
    "        for k in range(layer_shape[2]):\n",
    "            for i in range(layer_shape[0]):\n",
    "                writer.writerow(quantized[key][i,:,k,l].astype(int))\n",
    "with open(SAVE_PATH_W_TRUNC, 'w') as file:\n",
    "    writer = csv.writer(file, delimiter = ' ', lineterminator = '\\n')\n",
    "    for l in range(layer_shape[3]):\n",
    "        for k in range(layer_shape[2]):\n",
    "            for i in range(layer_shape[0]):\n",
    "                writer.writerow(quantized_and_dequantized[key][i,:,k,l].numpy())\n",
    "with open(SAVE_PATH_W_ORIGINAL, 'w') as file:\n",
    "    writer = csv.writer(file, delimiter = ' ', lineterminator = '\\n')\n",
    "    for l in range(layer_shape[3]):\n",
    "        for k in range(layer_shape[2]):\n",
    "            for i in range(layer_shape[0]):\n",
    "                writer.writerow(q_aware_model.layers[layer_index].trainable_variables[T_VARIABLES_KERNEL_INDEX][i,:,k,l].numpy())\n",
    "with open(SAVE_PATH_B_ORIGINAL, 'w') as file:\n",
    "    writer = csv.writer(file, delimiter = ' ', lineterminator = '\\n')\n",
    "    for l in range(layer_shape[3]):\n",
    "        writer.writerow([q_aware_model.layers[layer_index].trainable_variables[T_VARIABLES_BIAS_INDEX][l].numpy()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_master_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27b772b08f604b6b43e5a3a606000b08c03a565bb0332f867b1c8863c689a183"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
