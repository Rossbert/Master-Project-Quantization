{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from typing import Tuple, List\n",
    "from collections import OrderedDict\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(interpreter: tf.lite.Interpreter, test_images, test_labels) -> Tuple[float, float]:\n",
    "    \"\"\" Evaluate TFLite Model:\n",
    "    -\n",
    "    Receives the interpreter and returns a tuple of loss and accuracy.\n",
    "    \"\"\"\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    predictions = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis = 0).astype(np.float32)\n",
    "        test_image = np.expand_dims(test_image, axis = 3).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        predictions.append(np.copy(output()[0]))\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    predictions = np.array(predictions)\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()(test_labels, predictions)\n",
    "\n",
    "    loss = scce.numpy()\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return loss, accuracy\n",
    "\n",
    "def random_bit_flipper_uniform(value : int) -> Tuple[int, int]:\n",
    "    \"\"\" Random bit flipper with uniform distribution\n",
    "    -\n",
    "    Obtains a value and flips one bit at a random position according to a uniform distribution.\n",
    "    - All values are in 8 bits, MSB have higher probability of getting flipped\n",
    "    - It is assumed value is a signed 8 bit number \"\"\"\n",
    "    bit_pos = np.random.randint(8)\n",
    "    # Negative 2 Complement conversion\n",
    "    if value < 0:\n",
    "        value = (-value ^ 0xFF) + 1\n",
    "    flip_mask = 1 << bit_pos\n",
    "    flipped_value = value ^ flip_mask\n",
    "    # Negative back conversion 2 Complement\n",
    "    if flipped_value >= 128:\n",
    "        flipped_value = -((flipped_value ^ 0xFF) + 1)\n",
    "    return bit_pos, flipped_value\n",
    "\n",
    "\"\"\" Parameters to be tuned:\n",
    "- Output file name, if you don't update the name manually the previous file won't be deleted. New data will be appended to the end of the file instead.\n",
    "- Flag that enables training data to be saved, a False flag will decrease running time significantly.\n",
    "- Flag that enables laplacian related data to be saved.\n",
    "- Number of simulations per layer.\n",
    "- Total number of bits that will be flipped randomly from any weight in each layer.\n",
    "\"\"\"\n",
    "SAVE_FILE_NAME = 'Performance_Multiple_4.csv'\n",
    "SAVE_TRAINING_PERFORMANCE_FLAG = False\n",
    "SAVE_LAPLACIAN_DATA_FLAG = True\n",
    "N_SIMULATIONS_PER_LAYER = 2\n",
    "N_BITS_TO_FLIP = 20\n",
    "\n",
    "MODELS_DIR = \"./model/\"\n",
    "LOAD_PATH_Q_AWARE = MODELS_DIR + \"model_q_aware_final_01\"\n",
    "LOAD_TFLITE_PATH = MODELS_DIR + 'tflite_final_01.tflite'\n",
    "SAVE_NEW_TFLITE_PATH = MODELS_DIR + 'new_tflite_flip_01.tflite'\n",
    "OUTPUTS_DIR = \"./outputs/\"\n",
    "SAVE_DATA_PATH = OUTPUTS_DIR + SAVE_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUTS_DIR):\n",
    "    os.mkdir(OUTPUTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Q Aware model\n",
    "q_aware_model : tf.keras.Model\n",
    "with tfmot.quantization.keras.quantize_scope():\n",
    "    q_aware_model = tf.keras.models.load_model(LOAD_PATH_Q_AWARE)\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(LOAD_TFLITE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2891 - accuracy: 0.9115\n",
      "Q Aware model test accuracy :  91.150%\n",
      "Q Aware model test loss:  0.2890910804271698\n",
      "TFLite model test accuracy: 91.170%\n",
      "TFLite model test loss:  0.3396423\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy of both models in test set\n",
    "q_aware_test_loss, q_aware_test_acc = q_aware_model.evaluate(test_images, test_labels)\n",
    "print('Q Aware model test accuracy : ', \"{:0.3%}\".format(q_aware_test_acc))\n",
    "print('Q Aware model test loss: ', q_aware_test_loss)\n",
    "interpreter.allocate_tensors()\n",
    "tflite_test_loss, tflite_test_accuracy = evaluate_model(interpreter, test_images, test_labels)\n",
    "print('TFLite model test accuracy:', \"{:0.3%}\".format(tflite_test_accuracy))\n",
    "print('TFLite model test loss: ', tflite_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d/kernel\n",
      "[[-0.26310202  0.00828668  0.02278836  0.25688702  0.10358348]\n",
      " [-0.24652867 -0.15951855  0.1698769   0.19473694 -0.06422175]\n",
      " [ 0.25688702 -0.10565515 -0.07043677 -0.13465852 -0.01243002]\n",
      " [ 0.12637185  0.02278836  0.1636619  -0.1905936  -0.08079511]\n",
      " [-0.07250843  0.02486004  0.16780524 -0.00414334 -0.11808517]]\n",
      "[[-127.    4.   11.  124.   50.]\n",
      " [-119.  -77.   82.   94.  -31.]\n",
      " [ 124.  -51.  -34.  -65.   -6.]\n",
      " [  61.   11.   79.  -92.  -39.]\n",
      " [ -35.   12.   81.   -2.  -57.]]\n",
      "conv2d_1/kernel\n",
      "[[-0.05953331 -0.01488333  0.08929997  0.07069581  0.02232499]\n",
      " [ 0.16371661 -0.04092915  0.21580826  0.10046247  0.04837082]\n",
      " [ 0.11534579 -0.06697498  0.1041833   0.00372083  0.1748791 ]\n",
      " [ 0.10790413  0.01860416 -0.01860416 -0.10046247 -0.0111625 ]\n",
      " [-0.10046247  0.02976666 -0.07441664 -0.14511245 -0.18232077]]\n",
      "[[-16.  -4.  24.  19.   6.]\n",
      " [ 44. -11.  58.  27.  13.]\n",
      " [ 31. -18.  28.   1.  47.]\n",
      " [ 29.   5.  -5. -27.  -3.]\n",
      " [-27.   8. -20. -39. -49.]]\n",
      "conv2d_2/kernel\n",
      "[[-0.05118326  0.04739191 -0.01326973]\n",
      " [-0.01137406 -0.11942761 -0.03412218]\n",
      " [ 0.04739191 -0.06255732 -0.02085244]]\n",
      "[[-27.  25.  -7.]\n",
      " [ -6. -63. -18.]\n",
      " [ 25. -33. -11.]]\n",
      "dense_last/kernel\n",
      "[-0.0824218  -0.1236327   0.25756812  0.28847632 -0.18544905 -0.7314935\n",
      " -0.72119075 -0.14423816  0.15454088 -0.62846625  0.2472654   0.22665995\n",
      " -0.35029265 -0.1648436   0.29877904 -0.2472654   0.4533199  -0.2781736\n",
      "  0.36059538 -0.22665995 -0.57695264 -0.25756812  0.03090817 -0.8139153\n",
      " -0.2472654  -0.22665995 -0.4533199   0.21635723  0.03090817 -0.15454088\n",
      " -0.09272452  0.29877904 -0.25756812 -0.638769    0.15454088  0.4018063\n",
      "  0.1648436  -0.43271446 -0.7005853  -0.07211908  0.15454088 -0.29877904\n",
      " -0.02060545 -0.10302725 -0.33998993 -0.23696268 -0.06181635 -0.25756812\n",
      "  0.10302725  0.26787084 -0.0824218  -0.58725536  0.2472654   0.1648436\n",
      " -0.1236327   0.10302725  0.10302725  0.38120082  0.42241174  0.01030273\n",
      " -0.35029265 -0.30908176  0.0412109  -0.17514633 -0.03090817 -0.43271446\n",
      "  0.09272452 -0.23696268  0.0412109   0.15454088 -0.0824218  -0.19575179\n",
      "  0.1236327  -0.4533199   0.11332998 -0.18544905 -0.5460444   0.3296872\n",
      "  0.28847632 -0.19575179  0.02060545 -0.2781736  -0.03090817  0.17514633\n",
      " -0.52543896  0.0824218  -0.48422807 -0.36059538  0.14423816 -0.22665995\n",
      " -0.20605451 -0.3193845   0.2472654  -0.03090817 -0.18544905 -0.6181635 ]\n",
      "[ -8. -12.  25.  28. -18. -71. -70. -14.  15. -61.  24.  22. -34. -16.\n",
      "  29. -24.  44. -27.  35. -22. -56. -25.   3. -79. -24. -22. -44.  21.\n",
      "   3. -15.  -9.  29. -25. -62.  15.  39.  16. -42. -68.  -7.  15. -29.\n",
      "  -2. -10. -33. -23.  -6. -25.  10.  26.  -8. -57.  24.  16. -12.  10.\n",
      "  10.  37.  41.   1. -34. -30.   4. -17.  -3. -42.   9. -23.   4.  15.\n",
      "  -8. -19.  12. -44.  11. -18. -53.  32.  28. -19.   2. -27.  -3.  17.\n",
      " -51.   8. -47. -35.  14. -22. -20. -31.  24.  -3. -18. -60.]\n"
     ]
    }
   ],
   "source": [
    "# Quantification of values\n",
    "BIT_WIDTH = 8\n",
    "quantized_and_dequantized = OrderedDict()\n",
    "quantized = OrderedDict()\n",
    "layer_index_list = []\n",
    "keys_list = []\n",
    "layers_shapes = []\n",
    "\n",
    "layer : tfmot.quantization.keras.QuantizeWrapperV2\n",
    "for i, layer in enumerate(q_aware_model.layers):\n",
    "    quantizer : tfmot.quantization.keras.quantizers.Quantizer\n",
    "    weight : tf.Variable\n",
    "    if hasattr(layer, '_weight_vars'):\n",
    "        for weight, quantizer, quantizer_vars in layer._weight_vars:\n",
    "            min_var = quantizer_vars['min_var']\n",
    "            max_var = quantizer_vars['max_var']\n",
    "\n",
    "            key = weight.name[:-2]\n",
    "            layer_index_list.append(i)\n",
    "            keys_list.append(key)\n",
    "            layers_shapes.append(weight.numpy().shape)\n",
    "            quantized_and_dequantized[key] = quantizer(inputs = weight, training = False, weights = quantizer_vars)\n",
    "            quantized[key] = np.round(quantized_and_dequantized[key] / max_var * (2**(BIT_WIDTH-1)-1))\n",
    "\n",
    "for key in keys_list:\n",
    "    print(key)\n",
    "    kernel_idx = keys_list.index(key)\n",
    "    layer_index = layer_index_list[kernel_idx]\n",
    "\n",
    "    m_vars = {variable.name: variable for i, variable in enumerate(q_aware_model.layers[layer_index].non_trainable_variables) if keys_list[kernel_idx] in variable.name}\n",
    "    min_key = list(key for key in m_vars if \"min\" in key)[0]\n",
    "    max_key = list(key for key in m_vars if \"max\" in key)[0]\n",
    "    # Random position for weight change and max min variables identification\n",
    "    if \"dense\" not in key:\n",
    "        # It is a convolutional layer\n",
    "        kernel_row = np.random.randint(0, layers_shapes[kernel_idx][0])\n",
    "        kernel_column = np.random.randint(0, layers_shapes[kernel_idx][1])\n",
    "        in_channel = np.random.randint(0, layers_shapes[kernel_idx][2])\n",
    "        out_channel = np.random.randint(0, layers_shapes[kernel_idx][3])\n",
    "        position = (kernel_row, kernel_column, in_channel, out_channel)\n",
    "        kernel_position = (slice(None), slice(None), in_channel, out_channel)\n",
    "        value_position = (kernel_row, kernel_column)\n",
    "        # Convolutional layers max is divided per channels\n",
    "        min_var = m_vars[min_key][out_channel].numpy()\n",
    "        max_var = m_vars[max_key][out_channel].numpy()\n",
    "    else:\n",
    "        # It is a fully connected layer\n",
    "        kernel_row = None\n",
    "        kernel_column = None\n",
    "        in_channel = np.random.randint(0, layers_shapes[kernel_idx][0])\n",
    "        out_channel = np.random.randint(0, layers_shapes[kernel_idx][1])\n",
    "        position = (in_channel, out_channel)\n",
    "        kernel_position = (slice(None), out_channel)\n",
    "        value_position = (in_channel)\n",
    "        # Fully connected layer has only 1 max value for the kernel\n",
    "        min_var = m_vars[min_key].numpy()\n",
    "        max_var = m_vars[max_key].numpy()\n",
    "\n",
    "    print(quantized_and_dequantized[key][kernel_position].numpy())\n",
    "    print(quantized[key][kernel_position])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 32)\n",
      "-0.41804698\n",
      "0.41804698\n",
      "[[-0.11190058  0.02968932  0.13516179 -0.21755771 -0.41798258]\n",
      " [-0.23117375  0.02469435  0.18612708  0.08809777 -0.24172066]\n",
      " [ 0.01567534  0.130002   -0.09754552 -0.05695752  0.10985629]\n",
      " [ 0.15383062 -0.07598737 -0.00296063 -0.07511681 -0.0047392 ]\n",
      " [ 0.12786986  0.0541554  -0.06213173  0.0122421   0.03846057]]\n",
      "0.029690962\n",
      "[[-0.11191808  0.02962537  0.13496004 -0.21725276 -0.41804698]\n",
      " [-0.23041959  0.02633367  0.18762738  0.08887613 -0.24029471]\n",
      " [ 0.01645854  0.12837663 -0.09875125 -0.05595904  0.10862638]\n",
      " [ 0.1547103  -0.07570929 -0.00329171 -0.07570929 -0.00329171]\n",
      " [ 0.12837663  0.05266733 -0.06254246  0.01316683  0.0395005 ]]\n",
      "[[ -34.    9.   41.  -66. -127.]\n",
      " [ -70.    8.   57.   27.  -73.]\n",
      " [   5.   39.  -30.  -17.   33.]\n",
      " [  47.  -23.   -1.  -23.   -1.]\n",
      " [  39.   16.  -19.    4.   12.]]\n",
      "0.02968992057503783\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "LAYER_TARGET = 0\n",
    "key = keys_list[LAYER_TARGET]\n",
    "kernel_idx = keys_list.index(key)\n",
    "layer_index = layer_index_list[kernel_idx]\n",
    "\n",
    "m_vars = {variable.name: variable for i, variable in enumerate(q_aware_model.layers[layer_index].non_trainable_variables) if keys_list[kernel_idx] in variable.name}\n",
    "min_key = list(key for key in m_vars if \"min\" in key)[0]\n",
    "max_key = list(key for key in m_vars if \"max\" in key)[0]\n",
    "\n",
    "kernel_row = np.random.randint(0, layers_shapes[kernel_idx][0])\n",
    "kernel_column = np.random.randint(0, layers_shapes[kernel_idx][1])\n",
    "in_channel = 0\n",
    "out_channel = 0\n",
    "\n",
    "if \"dense\" not in key:\n",
    "    # It is a convolutional layer\n",
    "    position = (kernel_row, kernel_column, in_channel, out_channel)\n",
    "    kernel_position = (slice(None), slice(None), in_channel, out_channel)\n",
    "    value_position = (kernel_row, kernel_column)\n",
    "    # Convolutional layers max is divided per channels\n",
    "    min_var = m_vars[min_key][out_channel].numpy()\n",
    "    max_var = m_vars[max_key][out_channel].numpy()\n",
    "else:\n",
    "    # It is a fully connected layer\n",
    "    position = (in_channel, out_channel)\n",
    "    kernel_position = (slice(None), out_channel)\n",
    "    value_position = (in_channel)\n",
    "    # Fully connected layer has only 1 max value for the kernel\n",
    "    min_var = m_vars[min_key].numpy()\n",
    "    max_var = m_vars[max_key].numpy()\n",
    "    \n",
    "T_VARIABLES_KERNEL_INDEX = 0\n",
    "T_VARIABLES_BIAS_INDEX = 1\n",
    "print(layers_shapes[kernel_idx])\n",
    "print(min_var)\n",
    "print(max_var)\n",
    "print(q_aware_model.layers[layer_index].trainable_variables[T_VARIABLES_KERNEL_INDEX][kernel_position].numpy())\n",
    "print(q_aware_model.layers[layer_index].trainable_variables[T_VARIABLES_BIAS_INDEX][out_channel].numpy())\n",
    "print(quantized_and_dequantized[key][kernel_position].numpy())\n",
    "print(quantized[key][kernel_position])\n",
    "# From TFLite model\n",
    "QUANTIZED_BIAS = 2300\n",
    "SCALE_BIAS = 0.000012908661119581666\n",
    "false_quantized_bias = QUANTIZED_BIAS*SCALE_BIAS\n",
    "print(false_quantized_bias)\n",
    "ZERO = 7\n",
    "OUTPUT_SCALE = 0.03260320425033569\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight_attrs': ['kernel'], 'activation_attrs': ['activation'], 'quantize_output': False}\n",
      "<tensorflow_model_optimization.python.core.quantization.keras.quantizers.MovingAverageQuantizer object at 0x000001677C8220E0>\n"
     ]
    }
   ],
   "source": [
    "l = 2\n",
    "print(q_aware_model.layers[l].quantize_config.get_config())\n",
    "print(q_aware_model.layers[l].quantize_config.activation_quantizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_master_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27b772b08f604b6b43e5a3a606000b08c03a565bb0332f867b1c8863c689a183"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
